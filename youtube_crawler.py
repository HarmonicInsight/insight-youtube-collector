#!/usr/bin/env python3
"""
YouTube Transcript Crawler
- YouTubeå‹•ç”»ã®å­—å¹•ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã—ã¦JSONã«ä¿å­˜ã™ã‚‹ãƒ„ãƒ¼ãƒ«
- å˜ä¸€å‹•ç”»URLã€è¤‡æ•°URLä¸€æ‹¬ã€ãƒãƒ£ãƒ³ãƒãƒ«/ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã«å¯¾å¿œ
- æ—¥æœ¬èªå­—å¹•ã‚’å„ªå…ˆå–å¾—ï¼ˆãªã‘ã‚Œã°è‡ªå‹•ç”Ÿæˆå­—å¹•â†’è‹±èªã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

Usage:
  python youtube_crawler.py --url "https://www.youtube.com/watch?v=XXXXX"
  python youtube_crawler.py --url "URL1" "URL2" "URL3"
  python youtube_crawler.py --playlist "https://www.youtube.com/playlist?list=XXXXX"
  python youtube_crawler.py --channel "https://www.youtube.com/@channelname" --max 20
  python youtube_crawler.py --file urls.txt
  python youtube_crawler.py --search "å»ºè¨­DX AIæ´»ç”¨" --max 10
"""

import argparse
import json
import os
import re
import sys
import subprocess
from datetime import datetime, timezone
from pathlib import Path

try:
    from youtube_transcript_api import YouTubeTranscriptApi
    from youtube_transcript_api._errors import (
        TranscriptsDisabled,
        NoTranscriptFound,
        VideoUnavailable,
    )
except ImportError:
    print("ERROR: youtube-transcript-api ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã™")
    print("  pip install youtube-transcript-api")
    sys.exit(1)

try:
    import yt_dlp
except ImportError:
    print("ERROR: yt-dlp ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã™")
    print("  pip install yt-dlp")
    sys.exit(1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_video_id(url: str) -> str | None:
    """URLã‹ã‚‰YouTubeå‹•ç”»IDã‚’æŠ½å‡º"""
    patterns = [
        r'(?:v=|/v/|youtu\.be/)([a-zA-Z0-9_-]{11})',
        r'(?:embed/)([a-zA-Z0-9_-]{11})',
        r'^([a-zA-Z0-9_-]{11})$',  # bare 11-char ID only
    ]
    for pat in patterns:
        m = re.search(pat, url)
        if m:
            return m.group(1)
    return None


def get_video_metadata(video_id: str) -> dict:
    """yt-dlpã§å‹•ç”»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    url = f"https://www.youtube.com/watch?v={video_id}"
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'skip_download': True,
        'no_check_certificates': True,
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            return {
                'title': info.get('title', ''),
                'channel': info.get('channel', '') or info.get('uploader', ''),
                'channel_id': info.get('channel_id', ''),
                'upload_date': info.get('upload_date', ''),
                'duration_seconds': info.get('duration', 0),
                'view_count': info.get('view_count', 0),
                'like_count': info.get('like_count', 0),
                'description': info.get('description', ''),
                'tags': info.get('tags', []) or [],
                'categories': info.get('categories', []) or [],
                'thumbnail_url': info.get('thumbnail', ''),
            }
    except Exception as e:
        print(f"  âš  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def get_transcript(video_id: str, preferred_langs: list[str] = None) -> dict:
    """
    å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã€‚
    å„ªå…ˆé †ä½: æ‰‹å‹•å­—å¹•(ja) â†’ è‡ªå‹•ç”Ÿæˆå­—å¹•(ja) â†’ æ‰‹å‹•å­—å¹•(en) â†’ è‡ªå‹•ç”Ÿæˆå­—å¹•(en) â†’ æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®
    """
    if preferred_langs is None:
        preferred_langs = ['ja', 'en']

    try:
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

        # æ‰‹å‹•å­—å¹•ã‚’å„ªå…ˆçš„ã«æ¢ã™
        for lang in preferred_langs:
            try:
                transcript = transcript_list.find_transcript([lang])
                segments = transcript.fetch()
                return _format_transcript(segments, lang, is_generated=False)
            except NoTranscriptFound:
                pass

        # è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’æ¢ã™
        for lang in preferred_langs:
            try:
                transcript = transcript_list.find_generated_transcript([lang])
                segments = transcript.fetch()
                return _format_transcript(segments, lang, is_generated=True)
            except NoTranscriptFound:
                pass

        # ã©ã®è¨€èªã§ã‚‚ã„ã„ã®ã§å–å¾—
        for transcript in transcript_list:
            segments = transcript.fetch()
            return _format_transcript(
                segments,
                transcript.language_code,
                is_generated=transcript.is_generated,
            )

    except TranscriptsDisabled:
        return {'error': 'å­—å¹•ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™', 'segments': [], 'full_text': ''}
    except VideoUnavailable:
        return {'error': 'å‹•ç”»ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“', 'segments': [], 'full_text': ''}
    except Exception as e:
        return {'error': str(e), 'segments': [], 'full_text': ''}

    return {'error': 'å­—å¹•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“', 'segments': [], 'full_text': ''}


def _format_transcript(segments, language: str, is_generated: bool) -> dict:
    """ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢"""
    formatted_segments = []
    full_text_parts = []

    for seg in segments:
        # youtube_transcript_api v1.x ã§ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã‚ã‚Š
        if hasattr(seg, 'text'):
            text = seg.text
            start = getattr(seg, 'start', 0)
            duration = getattr(seg, 'duration', 0)
        elif isinstance(seg, dict):
            text = seg.get('text', '')
            start = seg.get('start', 0)
            duration = seg.get('duration', 0)
        else:
            # FetchedTranscriptSnippet ã®å ´åˆ
            text = str(seg)
            start = 0
            duration = 0

        clean_text = text.replace('\n', ' ').strip()
        if clean_text:
            formatted_segments.append({
                'start': round(start, 2),
                'duration': round(duration, 2),
                'text': clean_text,
            })
            full_text_parts.append(clean_text)

    return {
        'language': language,
        'is_generated': is_generated,
        'segment_count': len(formatted_segments),
        'segments': formatted_segments,
        'full_text': ' '.join(full_text_parts),
    }


def format_duration(seconds: int) -> str:
    """ç§’æ•°ã‚’HH:MM:SSå½¢å¼ã«"""
    if not seconds:
        return "ä¸æ˜"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h:02d}:{m:02d}:{s:02d}"
    return f"{m:02d}:{s:02d}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å‹•ç”»ãƒªã‚¹ãƒˆå–å¾—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_playlist_video_ids(playlist_url: str, max_videos: int = 50) -> list[str]:
    """ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‹ã‚‰å‹•ç”»IDãƒªã‚¹ãƒˆã‚’å–å¾—"""
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,
        'skip_download': True,
        'playlistend': max_videos,
    }
    video_ids = []
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(playlist_url, download=False)
            entries = info.get('entries', [])
            for entry in entries:
                vid = entry.get('id')
                if vid:
                    video_ids.append(vid)
    except Exception as e:
        print(f"  âš  ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    return video_ids[:max_videos]


def get_channel_video_ids(channel_url: str, max_videos: int = 20) -> list[str]:
    """ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰æœ€æ–°å‹•ç”»IDãƒªã‚¹ãƒˆã‚’å–å¾—"""
    # ãƒãƒ£ãƒ³ãƒãƒ«URLã‚’ /videos ã«æ­£è¦åŒ–
    if not channel_url.endswith('/videos'):
        channel_url = channel_url.rstrip('/') + '/videos'

    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,
        'skip_download': True,
        'playlistend': max_videos,
    }
    video_ids = []
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(channel_url, download=False)
            entries = info.get('entries', [])
            for entry in entries:
                vid = entry.get('id')
                if vid:
                    video_ids.append(vid)
    except Exception as e:
        print(f"  âš  ãƒãƒ£ãƒ³ãƒãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    return video_ids[:max_videos]


def search_youtube(query: str, max_results: int = 10) -> list[str]:
    """YouTubeæ¤œç´¢ã§å‹•ç”»IDãƒªã‚¹ãƒˆã‚’å–å¾—"""
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,
        'skip_download': True,
        'default_search': f'ytsearch{max_results}',
    }
    video_ids = []
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(query, download=False)
            entries = info.get('entries', [])
            for entry in entries:
                vid = entry.get('id')
                if vid:
                    video_ids.append(vid)
    except Exception as e:
        print(f"  âš  æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    return video_ids[:max_results]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_video(video_id: str, include_segments: bool = True) -> dict:
    """1å‹•ç”»åˆ†ã®å‡¦ç†: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ + ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå–å¾—"""
    url = f"https://www.youtube.com/watch?v={video_id}"
    print(f"  ğŸ“¥ å‡¦ç†ä¸­: {url}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    metadata = get_video_metadata(video_id)
    title = metadata.get('title', '(ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜)')
    print(f"     ã‚¿ã‚¤ãƒˆãƒ«: {title}")
    print(f"     å†ç”Ÿæ™‚é–“: {format_duration(metadata.get('duration_seconds', 0))}")

    # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå–å¾—
    transcript_data = get_transcript(video_id)

    if transcript_data.get('error'):
        print(f"     âš  å­—å¹•: {transcript_data['error']}")
    else:
        lang = transcript_data.get('language', '?')
        gen = '(è‡ªå‹•ç”Ÿæˆ)' if transcript_data.get('is_generated') else '(æ‰‹å‹•)'
        count = transcript_data.get('segment_count', 0)
        text_len = len(transcript_data.get('full_text', ''))
        print(f"     âœ… å­—å¹•å–å¾—: {lang} {gen} / {count}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ / {text_len}æ–‡å­—")

    # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
    result = {
        'video_id': video_id,
        'url': url,
        'crawled_at': datetime.now(timezone.utc).isoformat(),
        'metadata': metadata,
        'transcript': {
            'language': transcript_data.get('language', ''),
            'is_generated': transcript_data.get('is_generated', False),
            'segment_count': transcript_data.get('segment_count', 0),
            'full_text': transcript_data.get('full_text', ''),
            'error': transcript_data.get('error', None),
        },
    }

    if include_segments and transcript_data.get('segments'):
        result['transcript']['segments'] = transcript_data['segments']

    return result


def save_results(results: list[dict], output_path: str, pretty: bool = True):
    """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    output = {
        'crawl_info': {
            'tool': 'YouTube Transcript Crawler',
            'version': '1.0.0',
            'crawled_at': datetime.now(timezone.utc).isoformat(),
            'total_videos': len(results),
            'successful': sum(1 for r in results if not r['transcript'].get('error')),
            'failed': sum(1 for r in results if r['transcript'].get('error')),
        },
        'videos': results,
    }

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2 if pretty else None)

    file_size = os.path.getsize(output_path)
    print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: {output_path}")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
    print(f"   å‹•ç”»æ•°: {output['crawl_info']['total_videos']}")
    print(f"   æˆåŠŸ: {output['crawl_info']['successful']} / å¤±æ•—: {output['crawl_info']['failed']}")


def merge_json(existing_path: str, new_results: list[dict]) -> list[dict]:
    """æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ï¼ˆé‡è¤‡æ’é™¤ï¼‰"""
    existing_results = []
    if os.path.exists(existing_path):
        with open(existing_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            existing_results = data.get('videos', [])

    existing_ids = {r['video_id'] for r in existing_results}
    added = 0
    for r in new_results:
        if r['video_id'] not in existing_ids:
            existing_results.append(r)
            existing_ids.add(r['video_id'])
            added += 1

    print(f"   ğŸ“ ãƒãƒ¼ã‚¸: æ—¢å­˜{len(existing_results) - added}ä»¶ + æ–°è¦{added}ä»¶ = åˆè¨ˆ{len(existing_results)}ä»¶")
    return existing_results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description='YouTubeå‹•ç”»ã®å­—å¹•ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã—ã¦JSONã«ä¿å­˜ã™ã‚‹ãƒ„ãƒ¼ãƒ«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # å˜ä¸€å‹•ç”»
  python youtube_crawler.py --url "https://www.youtube.com/watch?v=XXXXX"

  # è¤‡æ•°å‹•ç”»
  python youtube_crawler.py --url "URL1" "URL2" "URL3"

  # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ
  python youtube_crawler.py --playlist "https://www.youtube.com/playlist?list=XXXXX"

  # ãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€æ–°å‹•ç”»
  python youtube_crawler.py --channel "https://www.youtube.com/@channelname" --max 20

  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
  python youtube_crawler.py --search "å»ºè¨­DX AIæ´»ç”¨" --max 10

  # URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰
  python youtube_crawler.py --file urls.txt

  # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜JSONã«ãƒãƒ¼ã‚¸ï¼‰
  python youtube_crawler.py --url "URL" --output data.json --append
        """,
    )

    source = parser.add_mutually_exclusive_group(required=True)
    source.add_argument('--url', nargs='+', help='YouTubeå‹•ç”»URLï¼ˆè¤‡æ•°å¯ï¼‰')
    source.add_argument('--playlist', help='YouTubeãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆURL')
    source.add_argument('--channel', help='YouTubeãƒãƒ£ãƒ³ãƒãƒ«URL')
    source.add_argument('--search', help='YouTubeæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')
    source.add_argument('--file', help='URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆ1è¡Œ1URLï¼‰')

    parser.add_argument('--output', '-o', default='youtube_data.json', help='å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (default: youtube_data.json)')
    parser.add_argument('--max', type=int, default=20, help='æœ€å¤§å–å¾—å‹•ç”»æ•° (default: 20)')
    parser.add_argument('--no-segments', action='store_true', help='ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆè©³ç´°ã‚’çœç•¥ï¼ˆfull_textã®ã¿ä¿å­˜ï¼‰')
    parser.add_argument('--compact', action='store_true', help='JSONã‚’åœ§ç¸®å‡ºåŠ›')
    parser.add_argument('--append', action='store_true', help='æ—¢å­˜JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ï¼ˆé‡è¤‡æ’é™¤ï¼‰')

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ¬ YouTube Transcript Crawler v1.0.0")
    print("=" * 60)

    # å‹•ç”»IDãƒªã‚¹ãƒˆæ§‹ç¯‰
    video_ids = []

    if args.url:
        for u in args.url:
            vid = extract_video_id(u.strip())
            if vid:
                video_ids.append(vid)
            else:
                print(f"  âš  ç„¡åŠ¹ãªURL: {u}")

    elif args.playlist:
        print(f"ğŸ“‹ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿: {args.playlist}")
        video_ids = get_playlist_video_ids(args.playlist, args.max)

    elif args.channel:
        print(f"ğŸ“º ãƒãƒ£ãƒ³ãƒãƒ«èª­ã¿è¾¼ã¿: {args.channel}")
        video_ids = get_channel_video_ids(args.channel, args.max)

    elif args.search:
        print(f"ğŸ” æ¤œç´¢: '{args.search}'")
        video_ids = search_youtube(args.search, args.max)

    elif args.file:
        with open(args.file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    vid = extract_video_id(line)
                    if vid:
                        video_ids.append(vid)

    if not video_ids:
        print("âŒ å‡¦ç†å¯¾è±¡ã®å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        sys.exit(1)

    # é‡è¤‡æ’é™¤
    seen = set()
    unique_ids = []
    for vid in video_ids:
        if vid not in seen:
            seen.add(vid)
            unique_ids.append(vid)
    video_ids = unique_ids[:args.max]

    print(f"\nğŸ¯ å‡¦ç†å¯¾è±¡: {len(video_ids)}å‹•ç”»\n")

    # å„å‹•ç”»ã‚’å‡¦ç†
    results = []
    for i, vid in enumerate(video_ids, 1):
        print(f"[{i}/{len(video_ids)}]")
        result = process_video(vid, include_segments=not args.no_segments)
        results.append(result)
        print()

    # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰
    if args.append:
        results = merge_json(args.output, results)

    # ä¿å­˜
    save_results(results, args.output, pretty=not args.compact)

    print("\nâœ… å®Œäº†!")


if __name__ == '__main__':
    main()
