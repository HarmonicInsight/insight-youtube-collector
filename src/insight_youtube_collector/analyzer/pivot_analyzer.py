"""
PIVOT Analyzer - PIVOTãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹YouTubeæ–‡å­—èµ·ã“ã—åˆ†æï¼ˆå®Œå…¨ç‰ˆï¼‰

PIVOTãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯:
- å¯¾è±¡è»¸ï¼ˆWhatï¼‰: Process / Tool / People ã®3å±¤ãƒ¢ãƒ‡ãƒ«
- å£°ã®åˆ†é¡ï¼ˆVoiceï¼‰: P(Pain) / I(Insecurity) / V(Vision) / O(Objection) / T(Traction)

å“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³:
- å‹•è©ã‚«ãƒ†ã‚´ãƒª: éšœå®³ç³»/å›°é›£ç³»/å–ªå¤±ç³»/é¡˜æœ›ç³»/æ‹’å¦ç³»/æˆåŠŸç³»
- å‰¯è©ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: ã€Œéå¸¸ã«ã€Ã—1.5 ã€œ ã€Œã»ã¨ã‚“ã©ã€Ã—0.4
- èªå°¾ç¢ºä¿¡åº¦: æ–­å®š1.0 ã€œ ä¼è0.4
- å¼·åº¦ã‚¹ã‚³ã‚¢: base Ã— degree Ã— certainty

ä½¿ç”¨ä¾‹:
    from insight_youtube_collector.analyzer import PIVOTAnalyzer, analyze_videos

    analyzer = PIVOTAnalyzer(domain="biz_analysis", use_morphology=True)
    results = analyzer.analyze_videos(collected_videos)

    for result in results:
        print(f"{result.video_id}: Score={result.total_score}")
        for item in result.pivot_result.items:
            print(f"  {item.pivot_voice}: {item.title} (intensity={item.intensity_score:.2f})")
"""

import re
import uuid
import json
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from pathlib import Path
from enum import Enum

# Import VideoData model
from ..models.video import VideoData


# ========================================
# PIVOTå®šç¾©
# ========================================

class PIVOT:
    """PIVOT Voiceå®šç¾©"""
    PAIN = "P"
    INSECURITY = "I"
    VISION = "V"
    OBJECTION = "O"
    TRACTION = "T"

    ALL = [PAIN, INSECURITY, VISION, OBJECTION, TRACTION]

    LABELS = {
        "P": "Pain",
        "I": "Insecurity",
        "V": "Vision",
        "O": "Objection",
        "T": "Traction",
    }

    SCORES = {
        "P": -2,  # ç¾åœ¨ã®è² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
        "I": -1,  # å°†æ¥ã®æ½œåœ¨ãƒªã‚¹ã‚¯
        "V": 1,   # æ”¹å–„ã¸ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
        "O": -1,  # å®Ÿè¡Œéšœå£
        "T": 2,   # æˆåŠŸã®åœŸå°
    }

    DESCRIPTIONS = {
        "P": "èª²é¡Œãƒ»å›°ã‚Šã”ã¨",
        "I": "ä¸å®‰ãƒ»å¿ƒé…",
        "V": "è¦æœ›ãƒ»ç†æƒ³åƒ",
        "O": "æ‘©æ“¦ãƒ»æŠµæŠ—",
        "T": "æˆåŠŸãƒ»å¼·ã¿",
    }


# ========================================
# å“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ - å‹•è©ã‚«ãƒ†ã‚´ãƒª
# ========================================

class VerbCategory(Enum):
    """å‹•è©ã‚«ãƒ†ã‚´ãƒª"""
    OBSTACLE = "éšœå®³ç³»"      # æ­¢ã¾ã‚‹ã€è½ã¡ã‚‹ã€æ¶ˆãˆã‚‹ã€å£Šã‚Œã‚‹ â†’ Pain
    DIFFICULTY = "å›°é›£ç³»"    # å›°ã‚‹ã€è©°ã¾ã‚‹ã€æ‚©ã‚€ã€è¿·ã† â†’ Pain
    LOSS = "å–ªå¤±ç³»"          # å¤±ã†ã€è¾ã‚ã‚‹ã€ãªããªã‚‹ã€æ¸›ã‚‹ â†’ Insecurity
    DESIRE = "é¡˜æœ›ç³»"        # æ¬²ã—ã„ã€ã—ãŸã„ã€ãªã‚ŠãŸã„ â†’ Vision
    REJECTION = "æ‹’å¦ç³»"     # å«ŒãŒã‚‹ã€åå¯¾ã™ã‚‹ã€æ‹’ã‚€ â†’ Objection
    SUCCESS = "æˆåŠŸç³»"       # ã§ãã‚‹ã€å›ã‚‹ã€ã†ã¾ãã„ã â†’ Traction
    NEUTRAL = "é€šå¸¸"


VERB_CATEGORY_DICT = {
    VerbCategory.OBSTACLE: [
        "æ­¢ã¾ã‚‹", "è½ã¡ã‚‹", "æ¶ˆãˆã‚‹", "å£Šã‚Œã‚‹", "é–“é•ãˆã‚‹",
        "å‹•ã‹ãªã„", "èµ·å‹•ã—ãªã„", "ãƒ•ãƒªãƒ¼ã‚ºã™ã‚‹", "ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹",
        "é…ã‚Œã‚‹", "æ»ã‚‹", "åœæ»ã™ã‚‹", "ä¸­æ–­ã™ã‚‹", "é€”åˆ‡ã‚Œã‚‹",
        "æ¼ã‚Œã‚‹", "æŠœã‘ã‚‹", "å¿˜ã‚Œã‚‹", "è¦‹è½ã¨ã™", "ãƒŸã‚¹ã‚‹",
    ],
    VerbCategory.DIFFICULTY: [
        "å›°ã‚‹", "è©°ã¾ã‚‹", "æ‚©ã‚€", "è¿·ã†", "åˆ†ã‹ã‚‰ãªã„",
        "è‹¦åŠ´ã™ã‚‹", "æ‰‹é–“å–ã‚‹", "ã¦ã“ãšã‚‹", "è¡Œãè©°ã¾ã‚‹",
        "è¿½ã„ã¤ã‹ãªã„", "é–“ã«åˆã‚ãªã„", "è¶³ã‚Šãªã„", "ä¸è¶³ã™ã‚‹",
        "ã¤ã¾ãšã", "ãƒãƒã‚‹", "æ²¼ã‚‹",
    ],
    VerbCategory.LOSS: [
        "å¤±ã†", "è¾ã‚ã‚‹", "ãªããªã‚‹", "æ¸›ã‚‹", "ã„ãªããªã‚‹",
        "é€€è·ã™ã‚‹", "é›¢è·ã™ã‚‹", "å»ã‚‹", "æŠœã‘ã‚‹",
        "å¿˜ã‚Œã‚‰ã‚Œã‚‹", "å¼•ãç¶™ã’ãªã„", "ä¼ã‚ã‚‰ãªã„",
        "é™³è…åŒ–ã™ã‚‹", "æ™‚ä»£é…ã‚Œã«ãªã‚‹", "å»ƒæ­¢ã•ã‚Œã‚‹",
    ],
    VerbCategory.DESIRE: [
        "æ¬²ã—ã„", "ã»ã—ã„", "ã—ãŸã„", "ã‚„ã‚ŠãŸã„", "ãªã‚ŠãŸã„",
        "ã§ãã‚‹ã‚ˆã†ã«ã—ãŸã„", "å®Ÿç¾ã—ãŸã„", "å°å…¥ã—ãŸã„",
        "æ”¹å–„ã—ãŸã„", "åŠ¹ç‡åŒ–ã—ãŸã„", "è‡ªå‹•åŒ–ã—ãŸã„",
        "å¤‰ãˆãŸã„", "è¦‹ãŸã„", "çŸ¥ã‚ŠãŸã„", "ä½¿ã„ãŸã„",
    ],
    VerbCategory.REJECTION: [
        "å«ŒãŒã‚‹", "åå¯¾ã™ã‚‹", "æ‹’ã‚€", "æ‹’å¦ã™ã‚‹", "ç„¡è¦–ã™ã‚‹",
        "ã‚„ã‚ŠãŸããªã„", "ã—ãŸããªã„", "ä½¿ã„ãŸããªã„",
        "èªã‚ãªã„", "ç´å¾—ã—ãªã„", "å—ã‘å…¥ã‚Œãªã„",
        "å«Œã ", "é¢å€’ãã•ã„", "ã ã‚‹ã„",
    ],
    VerbCategory.SUCCESS: [
        "ã§ãã‚‹", "å›ã‚‹", "ã†ã¾ãã„ã", "å®šç€ã™ã‚‹", "ä½¿ã„ã“ãªã™",
        "æ©Ÿèƒ½ã™ã‚‹", "å‹•ã", "æˆåŠŸã™ã‚‹", "é”æˆã™ã‚‹",
        "æ”¹å–„ã•ã‚ŒãŸ", "åŠ¹ç‡åŒ–ã•ã‚ŒãŸ", "ä¾¿åˆ©ã«ãªã£ãŸ",
        "åŠ©ã‹ã‚‹", "æ¥½ã«ãªã‚‹", "ã‚¹ãƒ ãƒ¼ã‚ºã«ãªã‚‹",
    ],
}

VERB_TO_CATEGORY = {}
for category, verbs in VERB_CATEGORY_DICT.items():
    for verb in verbs:
        VERB_TO_CATEGORY[verb] = category

VERB_CATEGORY_TO_PIVOT = {
    VerbCategory.OBSTACLE: "P",
    VerbCategory.DIFFICULTY: "P",
    VerbCategory.LOSS: "I",
    VerbCategory.DESIRE: "V",
    VerbCategory.REJECTION: "O",
    VerbCategory.SUCCESS: "T",
}


# ========================================
# å“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ - å½¢å®¹è©ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
# ========================================

class Sentiment(Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    ANXIETY = "anxiety"
    NEUTRAL = "neutral"


ADJECTIVE_SENTIMENT_DICT = {
    Sentiment.NEGATIVE: [
        "é…ã„", "é›£ã—ã„", "é¢å€’ãª", "ç…©é›‘ãª", "ä¸ä¾¿ãª", "åˆ†ã‹ã‚Šã«ãã„",
        "è¤‡é›‘ãª", "å¤§å¤‰ãª", "å³ã—ã„", "è¾›ã„", "ãã¤ã„", "é‡ã„",
        "æ‚ªã„", "ãƒ€ãƒ¡ãª", "ã¾ãšã„", "ã²ã©ã„", "æœ€æ‚ªãª",
        "ä½¿ã„ã«ãã„", "è¦‹ã¥ã‚‰ã„", "åˆ†ã‹ã‚Šã¥ã‚‰ã„",
        "å¤ã„", "æ™‚ä»£é…ã‚Œãª", "éåŠ¹ç‡ãª", "ç„¡é§„ãª",
    ],
    Sentiment.POSITIVE: [
        "æ—©ã„", "é€Ÿã„", "ç°¡å˜ãª", "ä¾¿åˆ©ãª", "ä½¿ã„ã‚„ã™ã„", "åˆ†ã‹ã‚Šã‚„ã™ã„",
        "ã‚·ãƒ³ãƒ—ãƒ«ãª", "æ¥½ãª", "å¿«é©ãª", "ã‚¹ãƒ ãƒ¼ã‚ºãª", "åŠ¹ç‡çš„ãª",
        "è‰¯ã„", "ã„ã„", "ç´ æ™´ã‚‰ã—ã„", "æœ€é«˜ãª", "å„ªç§€ãª",
        "è¦‹ã‚„ã™ã„", "æ“ä½œã—ã‚„ã™ã„", "ç›´æ„Ÿçš„ãª",
        "æ–°ã—ã„", "ãƒ¢ãƒ€ãƒ³ãª", "æœ€æ–°ãª",
    ],
    Sentiment.ANXIETY: [
        "ä¸å®‰ãª", "å¿ƒé…ãª", "å±ã†ã„", "æ€ªã—ã„", "å±é™ºãª",
        "ä¸ç¢ºã‹ãª", "æ›–æ˜§ãª", "è„†ã„", "è„†å¼±ãª",
        "å±äººçš„ãª", "ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãª",
    ],
}

ADJECTIVE_TO_SENTIMENT = {}
for sentiment, adjectives in ADJECTIVE_SENTIMENT_DICT.items():
    for adj in adjectives:
        ADJECTIVE_TO_SENTIMENT[adj] = sentiment


# ========================================
# å“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ - å‰¯è©å¼·åº¦ä¿‚æ•°
# ========================================

DEGREE_ADVERBS = {
    1.5: ["éå¸¸ã«", "æ¥µã‚ã¦", "å…¨ã", "çµ¶å¯¾ã«", "åˆ°åº•", "ã‚ã¡ã‚ƒãã¡ã‚ƒ", "ã‚ã£ã¡ã‚ƒ", "ã™ã”ã", "ã‚‚ã®ã™ã”ã", "å®Œå…¨ã«", "100%", "é–“é•ã„ãªã"],
    1.3: ["ã‹ãªã‚Š", "ã¨ã¦ã‚‚", "ç›¸å½“", "å¤§å¹…ã«", "å¤§ã„ã«", "ã ã„ã¶", "ãšã„ã¶ã‚“", "ãªã‹ãªã‹"],
    1.0: [],
    0.7: ["å°‘ã—", "å¤šå°‘", "ã‚„ã‚„", "ã¡ã‚‡ã£ã¨", "è‹¥å¹²", "ã‚ãšã‹ã«", "å¹¾åˆ†", "ã„ãã‚‰ã‹"],
    0.4: ["ã»ã¨ã‚“ã©", "ã‚ã¾ã‚Š", "ãŸã„ã—ã¦", "ãã‚Œã»ã©"],
}

ADVERB_TO_DEGREE = {}
for factor, adverbs in DEGREE_ADVERBS.items():
    for adv in adverbs:
        ADVERB_TO_DEGREE[adv] = factor


# ========================================
# å“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ - èªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºä¿¡åº¦
# ========================================

TAIL_PATTERNS = [
    # (pattern, certainty, pivot_tendency)
    # æ–­å®š (ç¢ºä¿¡åº¦ 1.0)
    (r"(?:ã§ã™|ã¾ã™|ã |ã§ã‚ã‚‹)$", 1.0, "P"),
    (r"(?:ã§ã™ã­|ã¾ã™ã­|ã ã­|ã ã‚ˆã­)$", 1.0, "P"),
    # çµŒé¨“ (ç¢ºä¿¡åº¦ 0.9)
    (r"(?:ã¾ã—ãŸ|ã—ãŸ|ã¦ã—ã¾ã£ãŸ|ã¡ã‚ƒã£ãŸ)$", 0.9, "P"),
    (r"(?:ã¦ã„ã‚‹|ã¦ã‚‹|ã¦ã„ãŸ|ã¦ãŸ)$", 0.9, "P"),
    # æ¨æ¸¬ (ç¢ºä¿¡åº¦ 0.6)
    (r"(?:ã‹ã‚‚ã—ã‚Œãªã„|ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“)$", 0.6, "I"),
    (r"(?:ã ã‚ã†|ã§ã—ã‚‡ã†)$", 0.6, "I"),
    (r"(?:ã¨æ€ã†|ã¨æ€ã„ã¾ã™)$", 0.6, "I"),
    (r"(?:æ°—ãŒã™ã‚‹|æ°—ãŒã—ã¾ã™)$", 0.6, "I"),
    # ä¼è (ç¢ºä¿¡åº¦ 0.4)
    (r"(?:ã‚‰ã—ã„|ã‚‰ã—ã„ã§ã™)$", 0.4, "I"),
    (r"(?:ãã†ã |ãã†ã§ã™)$", 0.4, "I"),
    # é¡˜æœ› (ç¢ºä¿¡åº¦ 0.8)
    (r"(?:ã¦ã»ã—ã„|ã¦ã»ã—ã„ã§ã™|ã¦ã„ãŸã ããŸã„)$", 0.8, "V"),
    (r"(?:ãŸã„|ãŸã„ã§ã™|ãŸã„ã¨æ€ã†)$", 0.8, "V"),
    (r"(?:ã°ã„ã„ã®ã«|ã¨ã„ã„ã®ã«)$", 0.8, "V"),
    # å¦å®šçš„é¡˜æœ› (ç¢ºä¿¡åº¦ 0.8)
    (r"(?:ãŸããªã„|ãŸããªã„ã§ã™)$", 0.8, "O"),
    (r"(?:ã¦ã»ã—ããªã„|ãªã„ã§ã»ã—ã„)$", 0.8, "O"),
]


# ========================================
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³è¾æ›¸
# ========================================

PIVOT_KEYWORDS = {
    "P": {
        "keywords": [
            "å›°ã£ã¦ã„ã‚‹", "å•é¡Œ", "èª²é¡Œ", "ã†ã¾ãã„ã‹ãªã„", "ã§ããªã„",
            "é›£ã—ã„", "éšœå®³", "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯", "ãƒˆãƒ©ãƒ–ãƒ«", "ã‚¨ãƒ©ãƒ¼",
            "é…ã‚Œ", "é…å»¶", "ä¸è¶³", "ãƒŸã‚¹", "å¤±æ•—", "æ­¢ã¾ã‚‹",
            "æ™‚é–“ãŒã‹ã‹ã‚‹", "æ‰‹é–“", "éåŠ¹ç‡", "ç„¡é§„", "é¢å€’",
            "ãƒã‚°", "ä¸å…·åˆ", "æ•…éšœ", "è½ã¡ã‚‹", "å‹•ã‹ãªã„",
        ],
        "patterns": [
            r"(.+?)(?:ã§|ã«)å›°ã£ã¦ã„ã‚‹",
            r"(.+?)(?:ãŒ|ã¯)(?:å•é¡Œ|èª²é¡Œ)(?:ã |ã§ã™|ã«ãªã£ã¦ã„ã‚‹)",
            r"(.+?)(?:ãŒ|ã¯)(?:ã†ã¾ãã„ã‹ãªã„|é›£ã—ã„|å³ã—ã„)",
            r"(.+?)(?:ãŒ|ã«)æ™‚é–“ãŒã‹ã‹ã‚‹",
            r"(.+?)(?:ã§|ãŒ)(?:ãƒŸã‚¹|ã‚¨ãƒ©ãƒ¼|ãƒˆãƒ©ãƒ–ãƒ«)ãŒ(?:èµ·ãã‚‹|ç™ºç”Ÿ)",
        ],
    },
    "I": {
        "keywords": [
            "å¿ƒé…", "ä¸å®‰", "æ‡¸å¿µ", "æ°—ã«ãªã‚‹", "æ°—ãŒã‹ã‚Š",
            "å¤§ä¸ˆå¤«ã‹", "ãƒªã‚¹ã‚¯", "å±ãªã„", "ã‚‚ã—ã‹ã—ãŸã‚‰",
            "ã‹ã‚‚ã—ã‚Œãªã„", "æã‚Œ", "å±äººåŒ–", "å¼•ç¶™ã",
            "è¾ã‚ãŸã‚‰", "ã„ãªããªã£ãŸã‚‰", "å°†æ¥", "ä»Šå¾Œ",
            "å…ˆè¡Œã", "è¦‹é€šã—", "ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹",
        ],
        "patterns": [
            r"(.+?)(?:ãŒ|ã‚’)(?:å¿ƒé…|ä¸å®‰|æ‡¸å¿µ)",
            r"(.+?)(?:ã‹ã‚‚ã—ã‚Œãªã„|æã‚ŒãŒã‚ã‚‹)",
            r"(?:è¾ã‚|ã„ãªããªã£)ãŸã‚‰(.+?)(?:ãŒ|ã¯|ã‚‚)(?:å›°ã‚‹|çµ‚ã‚ã‚‹|ã§ããªã„)",
            r"(.+?)(?:ãŒ|ã¯)å±äººåŒ–(?:ã—ã¦ã„ã‚‹|ã•ã‚Œã¦ã„ã‚‹)",
        ],
    },
    "V": {
        "keywords": [
            "ã—ã¦ã»ã—ã„", "æ¬²ã—ã„", "ã»ã—ã„", "ãŒã‚ã‚Œã°", "ã§ããŸã‚‰",
            "æœŸå¾…", "è¦æœ›", "å¸Œæœ›", "ç†æƒ³", "æ”¹å–„ã—ãŸã„",
            "åŠ¹ç‡åŒ–", "è‡ªå‹•åŒ–", "ã‚·ã‚¹ãƒ†ãƒ åŒ–", "ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–",
            "ã—ãŸã„", "ã§ãã‚‹ã‚ˆã†ã«", "ãªã‚Œã°ã„ã„", "ãªã‚‹ã¨ã„ã„",
            "å°å…¥ã—ãŸã„", "ä½¿ã„ãŸã„", "å®Ÿç¾ã—ãŸã„",
        ],
        "patterns": [
            r"(.+?)(?:ã—ã¦|ãŒ)(?:ã»ã—ã„|æ¬²ã—ã„|ãƒ›ã‚·ã‚¤)",
            r"(.+?)(?:ãŒã‚ã‚Œã°|ã§ãã‚Œã°)(?:ã„ã„|è‰¯ã„|å¬‰ã—ã„|åŠ©ã‹ã‚‹)",
            r"(.+?)(?:ã‚’|ãŒ)(?:åŠ¹ç‡åŒ–|è‡ªå‹•åŒ–|æ”¹å–„)(?:ã—ãŸã„|ã—ã¦ã»ã—ã„)",
            r"(.+?)(?:ã‚’|ãŒ)(?:å°å…¥|å®Ÿç¾)(?:ã—ãŸã„|ã—ã¦ã»ã—ã„)",
        ],
    },
    "O": {
        "keywords": [
            "åå¯¾", "æŠµæŠ—", "ç„¡ç†", "ã‚„ã‚ŠãŸããªã„",
            "å‰ã‚‚ãƒ€ãƒ¡ã ã£ãŸ", "å¤±æ•—ã—ãŸ", "ã†ã¾ãã„ã‹ãªã‹ã£ãŸ",
            "å«Œ", "é¢å€’", "ã‚¹ãƒˆãƒ¬ã‚¹", "å¯¾ç«‹", "è¡çª",
            "ã‚„ã‚‰ã•ã‚Œ", "å¼·åˆ¶", "ç´å¾—ã§ããªã„",
            "ç†è§£ã•ã‚Œãªã„", "å”åŠ›ã—ã¦ãã‚Œãªã„", "æŠ¼ã—ä»˜ã‘",
        ],
        "patterns": [
            r"(?:å‰|ä»¥å‰|éå»)(?:ã«|ã‚‚)(.+?)(?:ãŒ|ã§)(?:å¤±æ•—|ãƒ€ãƒ¡|ã†ã¾ãã„ã‹ãªã‹ã£ãŸ)",
            r"(.+?)(?:ã«|ã¯)(?:åå¯¾|æŠµæŠ—)(?:ãŒã‚ã‚‹|ã—ã¦ã„ã‚‹)",
            r"(.+?)(?:ã‚’|ã¯)(?:ã‚„ã‚ŠãŸããªã„|ã—ãŸããªã„)",
            r"(.+?)(?:ãŒ|ã¯)(?:ç´å¾—ã§ããªã„|ç†è§£ã§ããªã„)",
        ],
    },
    "T": {
        "keywords": [
            "ã†ã¾ãã„ã£ã¦ã„ã‚‹", "æˆåŠŸ", "é †èª¿", "å•é¡Œãªã„",
            "æº€è¶³", "è‰¯ã„", "ä¾¿åˆ©", "åŠ©ã‹ã£ã¦ã„ã‚‹", "åŠ¹ç‡çš„",
            "å¼·ã¿", "å¾—æ„", "å®šç€", "å›ã£ã¦ã„ã‚‹", "æ©Ÿèƒ½ã—ã¦ã„ã‚‹",
            "æ°—ã«å…¥ã£ã¦ã„ã‚‹", "ä½¿ã„ã‚„ã™ã„", "ã‚¹ãƒ ãƒ¼ã‚º",
            "ã†ã¾ã", "ã¡ã‚ƒã‚“ã¨", "ã—ã£ã‹ã‚Š", "å¿«é©",
        ],
        "patterns": [
            r"(.+?)(?:ã¯|ãŒ)(?:ã†ã¾ãã„ã£ã¦ã„ã‚‹|é †èª¿|æˆåŠŸ)",
            r"(.+?)(?:ã«|ã¯)(?:æº€è¶³|å•é¡Œãªã„)",
            r"(.+?)(?:ã¯|ãŒ)(?:ä¾¿åˆ©|åŠ©ã‹ã£ã¦ã„ã‚‹|åŠ¹ç‡çš„)",
            r"(.+?)(?:ã¯|ãŒ)(?:å¼·ã¿|å¾—æ„|å®šç€ã—ã¦ã„ã‚‹)",
            r"(.+?)(?:ã¯|ãŒ)(?:ã†ã¾ã|ã¡ã‚ƒã‚“ã¨)(?:å›ã£ã¦ã„ã‚‹|æ©Ÿèƒ½ã—ã¦ã„ã‚‹)",
        ],
    },
}


# ========================================
# å¯¾è±¡è»¸ï¼ˆLayerï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³
# ========================================

LAYER_PATTERNS = {
    "process": {
        "keywords": [
            "ç®¡ç†", "å‡¦ç†", "ä½œæ¥­", "æ¥­å‹™", "å¯¾å¿œ", "å ±å‘Š",
            "ç¢ºèª", "æ‰¿èª", "ç”³è«‹", "ç™ºæ³¨", "å—æ³¨", "è«‹æ±‚",
            "å…¥åŠ›", "å‡ºåŠ›", "é›†è¨ˆ", "åˆ†æ", "æ¤œæŸ»", "ç‚¹æ¤œ",
            "æ£šå¸", "åœ¨åº«", "å‡ºè·", "é…é€", "ä»•å…¥", "èª¿é”",
            "æ¡ç”¨", "è©•ä¾¡", "æ•™è‚²", "ç ”ä¿®", "å‹¤æ€ ", "çµ¦ä¸",
        ],
        "extraction_patterns": [
            r"(.{2,15}ç®¡ç†)",
            r"(.{2,15}å‡¦ç†)",
            r"(.{2,10}æ¥­å‹™)",
            r"(.{2,10}ä½œæ¥­)",
        ],
    },
    "tool": {
        "keywords": [
            "Excel", "ã‚¨ã‚¯ã‚»ãƒ«", "Word", "ãƒ¯ãƒ¼ãƒ‰", "PowerPoint", "ãƒ‘ãƒ¯ãƒ",
            "ãƒ¡ãƒ¼ãƒ«", "Slack", "Teams", "Zoom", "LINE", "ãƒãƒ£ãƒƒãƒˆ",
            "ã‚·ã‚¹ãƒ†ãƒ ", "ã‚½ãƒ•ãƒˆ", "ã‚¢ãƒ—ãƒª", "ãƒ„ãƒ¼ãƒ«",
            "ç´™", "å¸³ç¥¨", "ä¼ç¥¨", "FAX", "é›»è©±",
            "åŸºå¹¹", "ERP", "CRM", "SFA", "BIãƒ„ãƒ¼ãƒ«",
            "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ",
            "kintone", "ã‚­ãƒ³ãƒˆãƒ¼ãƒ³", "Salesforce", "SAP",
        ],
        "extraction_patterns": [
            r"(Excel|ã‚¨ã‚¯ã‚»ãƒ«|Word|ãƒ¯ãƒ¼ãƒ‰|PowerPoint|ãƒ‘ãƒ¯ãƒ)",
            r"(Slack|Teams|Zoom|LINE|ãƒ¡ãƒ¼ãƒ«)",
            r"(.{2,10}ã‚·ã‚¹ãƒ†ãƒ )",
            r"(ç´™|å¸³ç¥¨|ä¼ç¥¨|FAX|é›»è©±)",
        ],
    },
    "people": {
        "keywords": [
            "æ‹…å½“", "æ‹…å½“è€…", "è²¬ä»»è€…", "ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼", "ãƒªãƒ¼ãƒ€ãƒ¼",
            "éƒ¨é•·", "èª²é•·", "ä¿‚é•·", "ç¤¾é•·", "å½¹å“¡", "çµŒå–¶",
            "å–¶æ¥­", "çµŒç†", "ç·å‹™", "äººäº‹", "é–‹ç™º", "ç¾å ´",
            "éƒ¨ç½²", "ãƒãƒ¼ãƒ ", "ã‚°ãƒ«ãƒ¼ãƒ—", "å¤–æ³¨", "å”åŠ›ä¼šç¤¾",
            "æ–°äºº", "ãƒ™ãƒ†ãƒ©ãƒ³", "ãƒ‘ãƒ¼ãƒˆ", "æ´¾é£", "ã‚¢ãƒ«ãƒã‚¤ãƒˆ",
            "ä¸Šå¸", "éƒ¨ä¸‹", "åŒåƒš", "å¾Œè¼©", "å…ˆè¼©",
        ],
        "extraction_patterns": [
            r"(.{2,10}éƒ¨)",
            r"(.{2,10}èª²)",
            r"(æ‹…å½“è€…?|è²¬ä»»è€…|ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼|ãƒªãƒ¼ãƒ€ãƒ¼)",
            r"(å¤–æ³¨|å”åŠ›ä¼šç¤¾|ãƒ‘ãƒ¼ãƒˆ|æ´¾é£)",
        ],
    },
}


# ========================================
# æ¥­å‹™ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥PIVOTé‡ã¿
# ========================================

DOMAIN_PIVOT_WEIGHTS = {
    "requirements": {"P": 1.5, "I": 1.0, "V": 2.0, "O": 0.8, "T": 1.0},
    "biz_analysis": {"P": 2.0, "I": 1.2, "V": 1.0, "O": 1.0, "T": 1.5},
    "hr_evaluation": {"P": 1.0, "I": 2.0, "V": 1.5, "O": 1.8, "T": 1.2},
    "daily_concerns": {"P": 1.8, "I": 2.0, "V": 1.0, "O": 1.2, "T": 1.0},
    "customer_voice": {"P": 1.8, "I": 1.0, "V": 2.0, "O": 1.2, "T": 1.5},
    "retrospective": {"P": 1.5, "I": 1.2, "V": 1.5, "O": 1.2, "T": 1.8},
}


# ========================================
# å‹å®šç¾©
# ========================================

@dataclass
class MorphologyResult:
    """å“è©åˆ†è§£çµæœ"""
    verb_categories: List[VerbCategory] = field(default_factory=list)
    sentiment_score: float = 0.0
    degree_factor: float = 1.0
    certainty: float = 1.0
    pivot_tendency: Optional[str] = None
    matched_verbs: List[str] = field(default_factory=list)
    matched_adjectives: List[str] = field(default_factory=list)
    matched_adverbs: List[str] = field(default_factory=list)


@dataclass
class PIVOTInsight:
    """PIVOTåˆ†é¡ã•ã‚ŒãŸã‚¤ãƒ³ã‚µã‚¤ãƒˆ"""
    id: str
    pivot_voice: str
    pivot_label: str
    pivot_score: int
    target_layers: Dict[str, Optional[str]]  # process, tool, people
    title: str
    body: str
    confidence: float
    temperature: str
    matched_keywords: List[str] = field(default_factory=list)
    video_id: Optional[str] = None
    timestamp: Optional[float] = None
    # å“è©åˆ†è§£æƒ…å ±
    intensity_score: float = 0.0
    degree_factor: float = 1.0
    certainty: float = 1.0
    reasoning: str = ""


@dataclass
class PIVOTAnalysisResult:
    """PIVOTåˆ†æçµæœ"""
    items: List[PIVOTInsight]
    by_pivot: Dict[str, List[PIVOTInsight]]
    by_process: Dict[str, Dict[str, int]]
    by_tool: Dict[str, Dict[str, int]]
    total_score: int
    sentiment_index: float
    stats: Dict


@dataclass
class VideoAnalysisResult:
    """å‹•ç”»å˜ä½ã®åˆ†æçµæœ"""
    video_id: str
    video_title: str
    channel: str
    analyzed_at: str
    pivot_result: PIVOTAnalysisResult

    @property
    def total_score(self) -> int:
        return self.pivot_result.total_score

    @property
    def sentiment_index(self) -> float:
        return self.pivot_result.sentiment_index

    @property
    def pain_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("P", []))

    @property
    def insecurity_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("I", []))

    @property
    def vision_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("V", []))

    @property
    def objection_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("O", []))

    @property
    def traction_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("T", []))

    def to_dict(self) -> dict:
        return {
            "video_id": self.video_id,
            "video_title": self.video_title,
            "channel": self.channel,
            "analyzed_at": self.analyzed_at,
            "stats": {
                "total_insights": len(self.pivot_result.items),
                "total_score": self.total_score,
                "sentiment_index": round(self.sentiment_index, 2),
                "by_pivot": {
                    "P": self.pain_count,
                    "I": self.insecurity_count,
                    "V": self.vision_count,
                    "O": self.objection_count,
                    "T": self.traction_count,
                },
            },
            "items": [
                {
                    "id": item.id,
                    "pivot": item.pivot_voice,
                    "title": item.title,
                    "body": item.body,
                    "confidence": round(item.confidence, 2),
                    "temperature": item.temperature,
                    "keywords": item.matched_keywords,
                    "timestamp": item.timestamp,
                    "target_layers": item.target_layers,
                    "morphology": {
                        "intensity_score": round(item.intensity_score, 2),
                        "degree_factor": item.degree_factor,
                        "certainty": item.certainty,
                        "reasoning": item.reasoning,
                    },
                }
                for item in self.pivot_result.items
            ],
        }

    def to_mart_items(self, observed_at: Optional[str] = None) -> List[dict]:
        """PIVOT Martã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦å‡ºåŠ›"""
        observed_at = observed_at or datetime.now().strftime("%Y-%m-%d")
        marts = []

        for item in self.pivot_result.items:
            marts.append({
                "id": f"pivot_{item.id}",
                "mart_type": "pivot_insight",
                "pivot_voice": item.pivot_voice,
                "pivot_label": item.pivot_label,
                "pivot_score": item.pivot_score,
                "target_layers": item.target_layers,
                "title": item.title,
                "body": item.body,
                "confidence": item.confidence,
                "temperature": item.temperature,
                "keywords": {"surface": item.matched_keywords},
                "source_ref": {
                    "doc_id": self.video_id,
                    "doc_type": "youtube_transcript",
                    "channel": self.channel,
                    "title": self.video_title,
                    "timestamp": item.timestamp,
                },
                "source_time": {"observed_at": observed_at},
                "morphology": {
                    "intensity_score": round(item.intensity_score, 2),
                    "degree_factor": item.degree_factor,
                    "certainty": item.certainty,
                    "reasoning": item.reasoning,
                },
            })

        return marts


# ========================================
# å“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³
# ========================================

class MorphologyAnalyzer:
    """å“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""

    def __init__(self):
        self.tail_patterns = [(re.compile(p), c, t) for p, c, t in TAIL_PATTERNS]

    def analyze(self, text: str) -> MorphologyResult:
        result = MorphologyResult()

        # å‹•è©æŠ½å‡º
        for verb, category in VERB_TO_CATEGORY.items():
            if verb in text:
                result.verb_categories.append(category)
                result.matched_verbs.append(verb)

        # å½¢å®¹è©æŠ½å‡º
        positive_count = 0
        negative_count = 0
        anxiety_count = 0
        for adj, sentiment in ADJECTIVE_TO_SENTIMENT.items():
            if adj in text:
                result.matched_adjectives.append(adj)
                if sentiment == Sentiment.POSITIVE:
                    positive_count += 1
                elif sentiment == Sentiment.NEGATIVE:
                    negative_count += 1
                elif sentiment == Sentiment.ANXIETY:
                    anxiety_count += 1

        total_adj = positive_count + negative_count + anxiety_count
        if total_adj > 0:
            result.sentiment_score = (positive_count - negative_count - anxiety_count) / total_adj
            result.sentiment_score = max(-1.0, min(1.0, result.sentiment_score))

        # å‰¯è©æŠ½å‡º
        max_degree = 1.0
        for adv, factor in ADVERB_TO_DEGREE.items():
            if adv in text:
                result.matched_adverbs.append(adv)
                if factor > max_degree:
                    max_degree = factor
                elif factor < 1.0 and max_degree == 1.0:
                    max_degree = factor
        result.degree_factor = max_degree

        # èªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        for pattern, certainty, pivot_tendency in self.tail_patterns:
            if pattern.search(text):
                result.certainty = certainty
                result.pivot_tendency = pivot_tendency
                break

        return result


# ========================================
# PIVOT Analyzer
# ========================================

class PIVOTAnalyzer:
    """YouTubeæ–‡å­—èµ·ã“ã—ã®PIVOTåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆå®Œå…¨ç‰ˆï¼‰"""

    def __init__(
        self,
        domain: Optional[str] = None,
        min_confidence: float = 0.3,
        use_morphology: bool = True,
        split_by_sentence: bool = True,
    ):
        self.domain = domain
        self.min_confidence = min_confidence
        self.use_morphology = use_morphology
        self.split_by_sentence = split_by_sentence

        self.morphology_analyzer = MorphologyAnalyzer() if use_morphology else None
        self.weights = DOMAIN_PIVOT_WEIGHTS.get(domain, {p: 1.0 for p in PIVOT.ALL})

        self.pivot_patterns = {}
        for pivot, config in PIVOT_KEYWORDS.items():
            self.pivot_patterns[pivot] = [
                re.compile(p) for p in config.get("patterns", [])
            ]

        self.layer_extraction_patterns = {}
        for layer, config in LAYER_PATTERNS.items():
            self.layer_extraction_patterns[layer] = [
                re.compile(p) for p in config.get("extraction_patterns", [])
            ]

    def analyze_video(self, video: VideoData) -> VideoAnalysisResult:
        transcript_text = video.transcript.full_text if video.transcript else ""

        if not transcript_text:
            return VideoAnalysisResult(
                video_id=video.video_id,
                video_title=video.metadata.title,
                channel=video.metadata.channel,
                analyzed_at=datetime.now().isoformat(),
                pivot_result=PIVOTAnalysisResult(
                    items=[],
                    by_pivot={p: [] for p in PIVOT.ALL},
                    by_process={},
                    by_tool={},
                    total_score=0,
                    sentiment_index=0.0,
                    stats={"total": 0, "by_pivot": {p: 0 for p in PIVOT.ALL}},
                ),
            )

        sentences = self._split_sentences(transcript_text)
        timestamp_map = self._build_timestamp_map(video)

        items: List[PIVOTInsight] = []
        by_pivot: Dict[str, List[PIVOTInsight]] = {p: [] for p in PIVOT.ALL}
        by_process: Dict[str, Dict[str, int]] = {}
        by_tool: Dict[str, Dict[str, int]] = {}

        for sentence in sentences:
            insight = self._classify_sentence(sentence, video.video_id, timestamp_map)
            if insight and insight.confidence >= self.min_confidence:
                items.append(insight)
                by_pivot[insight.pivot_voice].append(insight)

                # å¯¾è±¡è»¸åˆ¥é›†è¨ˆ
                process = insight.target_layers.get("process")
                tool = insight.target_layers.get("tool")

                if process:
                    if process not in by_process:
                        by_process[process] = {p: 0 for p in PIVOT.ALL}
                    by_process[process][insight.pivot_voice] += 1

                if tool:
                    if tool not in by_tool:
                        by_tool[tool] = {p: 0 for p in PIVOT.ALL}
                    by_tool[tool][insight.pivot_voice] += 1

        # ãƒ‰ãƒ¡ã‚¤ãƒ³é‡ã¿ã‚’é©ç”¨ã—ã¦ã‚½ãƒ¼ãƒˆ
        items = self._apply_domain_weights(items)

        total_score = sum(item.pivot_score for item in items)
        sentiment_index = total_score / len(items) if items else 0.0

        stats = {
            "total": len(items),
            "by_pivot": {p: len(lst) for p, lst in by_pivot.items()},
            "domain": self.domain,
            "total_score": total_score,
            "sentiment_index": sentiment_index,
        }

        return VideoAnalysisResult(
            video_id=video.video_id,
            video_title=video.metadata.title,
            channel=video.metadata.channel,
            analyzed_at=datetime.now().isoformat(),
            pivot_result=PIVOTAnalysisResult(
                items=items,
                by_pivot=by_pivot,
                by_process=by_process,
                by_tool=by_tool,
                total_score=total_score,
                sentiment_index=sentiment_index,
                stats=stats,
            ),
        )

    def analyze_videos(self, videos: List[VideoData]) -> List[VideoAnalysisResult]:
        return [self.analyze_video(video) for video in videos]

    def _split_sentences(self, text: str) -> List[str]:
        if not self.split_by_sentence:
            return [text]
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]+', text)
        return [s.strip() for s in sentences if s.strip() and len(s.strip()) >= 10]

    def _build_timestamp_map(self, video: VideoData) -> Dict[str, float]:
        timestamp_map = {}
        if video.transcript and video.transcript.segments:
            for seg in video.transcript.segments:
                key = seg.text[:30] if len(seg.text) > 30 else seg.text
                timestamp_map[key] = seg.start
        return timestamp_map

    def _find_timestamp(self, text: str, timestamp_map: Dict[str, float]) -> Optional[float]:
        for key, timestamp in timestamp_map.items():
            if key in text or text[:30] in key:
                return timestamp
        return None

    def _extract_layers(self, text: str) -> Dict[str, Optional[str]]:
        """å¯¾è±¡è»¸ï¼ˆLayerï¼‰ã‚’æŠ½å‡º"""
        layers: Dict[str, Optional[str]] = {"process": None, "tool": None, "people": None}

        for layer, config in LAYER_PATTERNS.items():
            for kw in config["keywords"]:
                if kw in text:
                    for pattern in self.layer_extraction_patterns[layer]:
                        match = pattern.search(text)
                        if match:
                            layers[layer] = match.group(1)
                            break
                    if layers[layer]:
                        break
                    if not layers[layer]:
                        layers[layer] = kw
                        break

        return layers

    def _classify_sentence(
        self,
        text: str,
        video_id: str,
        timestamp_map: Dict[str, float],
    ) -> Optional[PIVOTInsight]:
        if not text.strip():
            return None

        # å“è©åˆ†è§£
        morph_result = None
        degree_factor = 1.0
        certainty = 1.0
        reasoning = ""

        if self.use_morphology and self.morphology_analyzer:
            morph_result = self.morphology_analyzer.analyze(text)
            degree_factor = morph_result.degree_factor
            certainty = morph_result.certainty

            # å“è©åˆ†è§£ã«ã‚ˆã‚‹PIVOTæ¨å®š
            morph_pivot, morph_conf, morph_reason = self._infer_pivot_from_morphology(morph_result, text)
            if morph_pivot and morph_conf >= 0.6:
                pivot_voice = morph_pivot
                confidence = morph_conf
                matched_keywords = morph_result.matched_verbs + morph_result.matched_adjectives
                reasoning = morph_reason
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
                pivot_result = self._classify_pivot(text)
                if not pivot_result:
                    return None
                pivot_voice, confidence, matched_keywords = pivot_result
                reasoning = "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹"
        else:
            pivot_result = self._classify_pivot(text)
            if not pivot_result:
                return None
            pivot_voice, confidence, matched_keywords = pivot_result
            reasoning = "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹"

        # å¯¾è±¡è»¸æŠ½å‡º
        target_layers = self._extract_layers(text)

        # æ¸©åº¦æ„Ÿåˆ¤å®š
        temperature = self._detect_temperature(text)

        # å¼·åº¦ã‚¹ã‚³ã‚¢ç®—å‡º
        base_score = PIVOT.SCORES[pivot_voice]
        intensity_score = base_score * degree_factor * certainty

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        timestamp = self._find_timestamp(text, timestamp_map)

        return PIVOTInsight(
            id=str(uuid.uuid4()),
            pivot_voice=pivot_voice,
            pivot_label=PIVOT.LABELS[pivot_voice],
            pivot_score=PIVOT.SCORES[pivot_voice],
            target_layers=target_layers,
            title=self._truncate(text, 50),
            body=text,
            confidence=confidence,
            temperature=temperature,
            matched_keywords=matched_keywords,
            video_id=video_id,
            timestamp=timestamp,
            intensity_score=intensity_score,
            degree_factor=degree_factor,
            certainty=certainty,
            reasoning=reasoning,
        )

    def _infer_pivot_from_morphology(
        self,
        morph: MorphologyResult,
        text: str,
    ) -> Tuple[Optional[str], float, str]:
        """å“è©åˆ†è§£çµæœã‹ã‚‰PIVOTã‚’æ¨å®š"""
        reasons = []

        # å‹•è©ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ã®åˆ¤å®š
        for cat in morph.verb_categories:
            if cat in VERB_CATEGORY_TO_PIVOT:
                pivot = VERB_CATEGORY_TO_PIVOT[cat]
                if cat == VerbCategory.OBSTACLE or cat == VerbCategory.DIFFICULTY:
                    if morph.sentiment_score < 0 and morph.certainty >= 0.9:
                        reasons.append(f"{cat.value}å‹•è©+ãƒã‚¬ãƒ†ã‚£ãƒ–å½¢å®¹è©+é«˜ç¢ºä¿¡åº¦")
                        return "P", 0.9, "; ".join(reasons)
                elif cat == VerbCategory.LOSS:
                    reasons.append(f"{cat.value}å‹•è©")
                    return "I", 0.85, "; ".join(reasons)
                elif cat == VerbCategory.DESIRE:
                    reasons.append(f"{cat.value}å‹•è©")
                    return "V", 0.85, "; ".join(reasons)
                elif cat == VerbCategory.REJECTION:
                    reasons.append(f"{cat.value}å‹•è©")
                    return "O", 0.85, "; ".join(reasons)
                elif cat == VerbCategory.SUCCESS:
                    if morph.sentiment_score > 0:
                        reasons.append(f"{cat.value}å‹•è©+ãƒã‚¸ãƒ†ã‚£ãƒ–å½¢å®¹è©")
                        return "T", 0.9, "; ".join(reasons)
                    else:
                        reasons.append(f"{cat.value}å‹•è©")
                        return "T", 0.7, "; ".join(reasons)

        # èªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®åˆ¤å®š
        if morph.pivot_tendency:
            if morph.certainty <= 0.6 and morph.pivot_tendency == "I":
                reasons.append("æ¨æ¸¬/ä¼èèªå°¾")
                return "I", 0.75, "; ".join(reasons)
            elif morph.pivot_tendency == "V":
                reasons.append("é¡˜æœ›èªå°¾")
                return "V", 0.8, "; ".join(reasons)
            elif morph.pivot_tendency == "O":
                reasons.append("å¦å®šçš„é¡˜æœ›èªå°¾")
                return "O", 0.8, "; ".join(reasons)

        # å½¢å®¹è©ã®ã¿ã§ã®åˆ¤å®š
        if morph.sentiment_score < -0.5:
            reasons.append("ãƒã‚¬ãƒ†ã‚£ãƒ–å½¢å®¹è©å„ªä½")
            return "P", 0.6, "; ".join(reasons)
        if morph.sentiment_score > 0.5:
            reasons.append("ãƒã‚¸ãƒ†ã‚£ãƒ–å½¢å®¹è©å„ªä½")
            return "T", 0.6, "; ".join(reasons)

        return None, 0.0, ""

    def _classify_pivot(self, text: str) -> Optional[Tuple[str, float, List[str]]]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰/ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®PIVOTåˆ†é¡"""
        scores: Dict[str, Tuple[float, List[str]]] = {}

        for pivot in PIVOT.ALL:
            config = PIVOT_KEYWORDS[pivot]
            keywords = config["keywords"]
            patterns = self.pivot_patterns[pivot]

            matched_kw = [kw for kw in keywords if kw in text]
            kw_score = min(len(matched_kw) * 0.2, 0.6)

            pat_score = 0.0
            for pattern in patterns:
                if pattern.search(text):
                    pat_score = 0.35
                    break

            total_score = min(kw_score + pat_score, 0.95)

            if total_score > 0:
                scores[pivot] = (total_score, matched_kw)

        if not scores:
            return None

        best_pivot = max(scores.keys(), key=lambda p: scores[p][0])
        confidence, matched_keywords = scores[best_pivot]

        return best_pivot, confidence, matched_keywords

    def _detect_temperature(self, text: str) -> str:
        high_indicators = ["çµ¶å¯¾", "æœ¬å½“ã«", "éå¸¸ã«", "ã¨ã¦ã‚‚", "ã™ã”ã", "ã‚ã¡ã‚ƒãã¡ã‚ƒ", "ã„ã¤ã‚‚", "æ¯å›", "å¿…ãš"]
        medium_indicators = ["ã‹ãªã‚Š", "çµæ§‹", "ã‚ã‚Šã¨", "æ™‚ã€…", "ãŸã¾ã«", "ã‚ˆã"]
        low_indicators = ["å°‘ã—", "ã¡ã‚‡ã£ã¨", "å¤šå°‘", "è‹¥å¹²", "ãŸã¶ã‚“", "ãŠãã‚‰ã"]

        if any(ind in text for ind in high_indicators):
            return "high"
        elif any(ind in text for ind in medium_indicators):
            return "medium"
        elif any(ind in text for ind in low_indicators):
            return "low"
        return "medium"

    def _apply_domain_weights(self, items: List[PIVOTInsight]) -> List[PIVOTInsight]:
        def weighted_score(item: PIVOTInsight) -> float:
            weight = self.weights.get(item.pivot_voice, 1.0)
            return abs(item.intensity_score) * weight

        return sorted(items, key=weighted_score, reverse=True)

    def _truncate(self, text: str, max_len: int) -> str:
        text = text.replace("\n", " ").strip()
        if len(text) <= max_len:
            return text
        return text[:max_len] + "..."


# ========================================
# ä¾¿åˆ©é–¢æ•°
# ========================================

def analyze_video(video: VideoData, domain: Optional[str] = None) -> VideoAnalysisResult:
    analyzer = PIVOTAnalyzer(domain=domain)
    return analyzer.analyze_video(video)


def analyze_videos(videos: List[VideoData], domain: Optional[str] = None) -> List[VideoAnalysisResult]:
    analyzer = PIVOTAnalyzer(domain=domain)
    return analyzer.analyze_videos(videos)


def save_analysis_results(
    results: List[VideoAnalysisResult],
    output_path: str,
    format: str = "json",
) -> None:
    path = Path(output_path)
    path.parent.mkdir(parents=True, exist_ok=True)

    if format == "jsonl":
        with open(path, "w", encoding="utf-8") as f:
            for result in results:
                for mart in result.to_mart_items():
                    f.write(json.dumps(mart, ensure_ascii=False) + "\n")
    else:
        data = {
            "analyzed_at": datetime.now().isoformat(),
            "total_videos": len(results),
            "results": [r.to_dict() for r in results],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


def print_analysis_summary(results: List[VideoAnalysisResult]) -> None:
    print("\n" + "=" * 60)
    print("PIVOTåˆ†æã‚µãƒãƒªãƒ¼ï¼ˆå“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ä»˜ãï¼‰")
    print("=" * 60)

    total_insights = sum(len(r.pivot_result.items) for r in results)
    total_score = sum(r.total_score for r in results)

    # å¼·åº¦ã‚¹ã‚³ã‚¢é›†è¨ˆ
    total_intensity = sum(
        abs(item.intensity_score)
        for r in results
        for item in r.pivot_result.items
    )
    avg_intensity = total_intensity / total_insights if total_insights > 0 else 0

    print(f"\nğŸ“Š åˆ†æå‹•ç”»æ•°: {len(results)}")
    print(f"ğŸ“ ç·ã‚¤ãƒ³ã‚µã‚¤ãƒˆæ•°: {total_insights}")
    print(f"ğŸ“ˆ ç·åˆã‚¹ã‚³ã‚¢: {total_score}")
    print(f"âš¡ å¹³å‡å¼·åº¦ã‚¹ã‚³ã‚¢: {avg_intensity:.2f}")

    # PIVOTåˆ¥é›†è¨ˆ
    pivot_totals = {p: 0 for p in PIVOT.ALL}
    pivot_intensity = {p: 0.0 for p in PIVOT.ALL}
    for result in results:
        for pivot in PIVOT.ALL:
            items = result.pivot_result.by_pivot.get(pivot, [])
            pivot_totals[pivot] += len(items)
            pivot_intensity[pivot] += sum(abs(i.intensity_score) for i in items)

    print("\nğŸ“‹ PIVOTåˆ†å¸ƒï¼ˆå¼·åº¦ã‚¹ã‚³ã‚¢ä»˜ãï¼‰:")
    print("-" * 50)
    pivot_labels = {
        "P": "Pain (èª²é¡Œ)",
        "I": "Insecurity (ä¸å®‰)",
        "V": "Vision (è¦æœ›)",
        "O": "Objection (æŠµæŠ—)",
        "T": "Traction (æˆåŠŸ)",
    }
    for pivot, label in pivot_labels.items():
        count = pivot_totals[pivot]
        intensity = pivot_intensity[pivot]
        bar = "â–ˆ" * min(count, 20)
        print(f"  {pivot} {label:20} {count:3}ä»¶ (å¼·åº¦:{intensity:6.1f}) {bar}")

    # å¯¾è±¡è»¸ï¼ˆProcessï¼‰åˆ¥é›†è¨ˆ
    process_totals: Dict[str, int] = {}
    for result in results:
        for process, counts in result.pivot_result.by_process.items():
            if process not in process_totals:
                process_totals[process] = 0
            process_totals[process] += sum(counts.values())

    if process_totals:
        print("\nğŸ”§ å¯¾è±¡è»¸ï¼ˆProcessï¼‰åˆ¥:")
        print("-" * 40)
        for process, count in sorted(process_totals.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  {process}: {count}ä»¶")

    # ä¸Šä½å‹•ç”»
    if results:
        print("\nğŸ¯ èª²é¡ŒãŒå¤šã„å‹•ç”» (Top 5):")
        print("-" * 40)
        sorted_by_pain = sorted(results, key=lambda r: r.pain_count, reverse=True)[:5]
        for r in sorted_by_pain:
            print(f"  [{r.pain_count}P] {r.video_title[:40]}")

    print("\n" + "=" * 60)
