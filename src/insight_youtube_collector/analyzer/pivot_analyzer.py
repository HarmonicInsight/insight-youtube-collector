"""
PIVOT Analyzer - PIVOTãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹YouTubeæ–‡å­—èµ·ã“ã—åˆ†æ

PIVOTãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯:
- å¯¾è±¡è»¸ï¼ˆWhatï¼‰: Process / Tool / People ã®3å±¤ãƒ¢ãƒ‡ãƒ«
- å£°ã®åˆ†é¡ï¼ˆVoiceï¼‰: P(Pain) / I(Insecurity) / V(Vision) / O(Objection) / T(Traction)

ä½¿ç”¨ä¾‹:
    from insight_youtube_collector.analyzer import PIVOTAnalyzer, analyze_videos

    analyzer = PIVOTAnalyzer(domain="biz_analysis")
    results = analyzer.analyze_videos(collected_videos)

    for result in results:
        print(f"{result.video_id}: {result.total_score} (P:{result.pain_count}, V:{result.vision_count})")
"""

import re
import uuid
import json
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from pathlib import Path

# Import VideoData model
from ..models.video import VideoData


# ========================================
# PIVOTå®šç¾©
# ========================================

class PIVOT:
    """PIVOT Voiceå®šç¾©"""
    PAIN = "P"
    INSECURITY = "I"
    VISION = "V"
    OBJECTION = "O"
    TRACTION = "T"

    ALL = [PAIN, INSECURITY, VISION, OBJECTION, TRACTION]

    LABELS = {
        "P": "Pain",
        "I": "Insecurity",
        "V": "Vision",
        "O": "Objection",
        "T": "Traction",
    }

    SCORES = {
        "P": -2,  # ç¾åœ¨ã®è² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
        "I": -1,  # å°†æ¥ã®æ½œåœ¨ãƒªã‚¹ã‚¯
        "V": 1,   # æ”¹å–„ã¸ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
        "O": -1,  # å®Ÿè¡Œéšœå£
        "T": 2,   # æˆåŠŸã®åœŸå°
    }

    DESCRIPTIONS = {
        "P": "èª²é¡Œãƒ»å›°ã‚Šã”ã¨",
        "I": "ä¸å®‰ãƒ»å¿ƒé…",
        "V": "è¦æœ›ãƒ»ç†æƒ³åƒ",
        "O": "æ‘©æ“¦ãƒ»æŠµæŠ—",
        "T": "æˆåŠŸãƒ»å¼·ã¿",
    }


# ========================================
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³è¾æ›¸
# ========================================

PIVOT_KEYWORDS = {
    "P": {  # Pain
        "keywords": [
            "å›°ã£ã¦ã„ã‚‹", "å•é¡Œ", "èª²é¡Œ", "ã†ã¾ãã„ã‹ãªã„", "ã§ããªã„",
            "é›£ã—ã„", "éšœå®³", "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯", "ãƒˆãƒ©ãƒ–ãƒ«", "ã‚¨ãƒ©ãƒ¼",
            "é…ã‚Œ", "é…å»¶", "ä¸è¶³", "ãƒŸã‚¹", "å¤±æ•—", "æ­¢ã¾ã‚‹",
            "æ™‚é–“ãŒã‹ã‹ã‚‹", "æ‰‹é–“", "éåŠ¹ç‡", "ç„¡é§„", "é¢å€’",
            "ãƒã‚°", "ä¸å…·åˆ", "æ•…éšœ", "è½ã¡ã‚‹", "å‹•ã‹ãªã„",
        ],
        "patterns": [
            r"(.+?)(?:ã§|ã«)å›°ã£ã¦ã„ã‚‹",
            r"(.+?)(?:ãŒ|ã¯)(?:å•é¡Œ|èª²é¡Œ)(?:ã |ã§ã™|ã«ãªã£ã¦ã„ã‚‹)",
            r"(.+?)(?:ãŒ|ã¯)(?:ã†ã¾ãã„ã‹ãªã„|é›£ã—ã„|å³ã—ã„)",
            r"(.+?)(?:ãŒ|ã«)æ™‚é–“ãŒã‹ã‹ã‚‹",
        ],
    },
    "I": {  # Insecurity
        "keywords": [
            "å¿ƒé…", "ä¸å®‰", "æ‡¸å¿µ", "æ°—ã«ãªã‚‹", "æ°—ãŒã‹ã‚Š",
            "å¤§ä¸ˆå¤«ã‹", "ãƒªã‚¹ã‚¯", "å±ãªã„", "ã‚‚ã—ã‹ã—ãŸã‚‰",
            "ã‹ã‚‚ã—ã‚Œãªã„", "æã‚Œ", "å±äººåŒ–", "å¼•ç¶™ã",
            "è¾ã‚ãŸã‚‰", "ã„ãªããªã£ãŸã‚‰", "å°†æ¥", "ä»Šå¾Œ",
        ],
        "patterns": [
            r"(.+?)(?:ãŒ|ã‚’)(?:å¿ƒé…|ä¸å®‰|æ‡¸å¿µ)",
            r"(.+?)(?:ã‹ã‚‚ã—ã‚Œãªã„|æã‚ŒãŒã‚ã‚‹)",
            r"(?:è¾ã‚|ã„ãªããªã£)ãŸã‚‰(.+?)(?:ãŒ|ã¯|ã‚‚)(?:å›°ã‚‹|çµ‚ã‚ã‚‹|ã§ããªã„)",
        ],
    },
    "V": {  # Vision
        "keywords": [
            "ã—ã¦ã»ã—ã„", "æ¬²ã—ã„", "ã»ã—ã„", "ãŒã‚ã‚Œã°", "ã§ããŸã‚‰",
            "æœŸå¾…", "è¦æœ›", "å¸Œæœ›", "ç†æƒ³", "æ”¹å–„ã—ãŸã„",
            "åŠ¹ç‡åŒ–", "è‡ªå‹•åŒ–", "ã‚·ã‚¹ãƒ†ãƒ åŒ–", "ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–",
            "ã—ãŸã„", "ã§ãã‚‹ã‚ˆã†ã«", "ãªã‚Œã°ã„ã„", "ãªã‚‹ã¨ã„ã„",
            "å°å…¥ã—ãŸã„", "ä½¿ã„ãŸã„", "å®Ÿç¾ã—ãŸã„",
        ],
        "patterns": [
            r"(.+?)(?:ã—ã¦|ãŒ)(?:ã»ã—ã„|æ¬²ã—ã„|ãƒ›ã‚·ã‚¤)",
            r"(.+?)(?:ãŒã‚ã‚Œã°|ã§ãã‚Œã°)(?:ã„ã„|è‰¯ã„|å¬‰ã—ã„|åŠ©ã‹ã‚‹)",
            r"(.+?)(?:ã‚’|ãŒ)(?:åŠ¹ç‡åŒ–|è‡ªå‹•åŒ–|æ”¹å–„)(?:ã—ãŸã„|ã—ã¦ã»ã—ã„)",
        ],
    },
    "O": {  # Objection
        "keywords": [
            "åå¯¾", "æŠµæŠ—", "ç„¡ç†", "ã‚„ã‚ŠãŸããªã„",
            "å‰ã‚‚ãƒ€ãƒ¡ã ã£ãŸ", "å¤±æ•—ã—ãŸ", "ã†ã¾ãã„ã‹ãªã‹ã£ãŸ",
            "å«Œ", "é¢å€’", "ã‚¹ãƒˆãƒ¬ã‚¹", "å¯¾ç«‹", "è¡çª",
            "ã‚„ã‚‰ã•ã‚Œ", "å¼·åˆ¶", "ç´å¾—ã§ããªã„",
        ],
        "patterns": [
            r"(?:å‰|ä»¥å‰|éå»)(?:ã«|ã‚‚)(.+?)(?:ãŒ|ã§)(?:å¤±æ•—|ãƒ€ãƒ¡|ã†ã¾ãã„ã‹ãªã‹ã£ãŸ)",
            r"(.+?)(?:ã«|ã¯)(?:åå¯¾|æŠµæŠ—)(?:ãŒã‚ã‚‹|ã—ã¦ã„ã‚‹)",
            r"(.+?)(?:ã‚’|ã¯)(?:ã‚„ã‚ŠãŸããªã„|ã—ãŸããªã„)",
        ],
    },
    "T": {  # Traction
        "keywords": [
            "ã†ã¾ãã„ã£ã¦ã„ã‚‹", "æˆåŠŸ", "é †èª¿", "å•é¡Œãªã„",
            "æº€è¶³", "è‰¯ã„", "ä¾¿åˆ©", "åŠ©ã‹ã£ã¦ã„ã‚‹", "åŠ¹ç‡çš„",
            "å¼·ã¿", "å¾—æ„", "å®šç€", "å›ã£ã¦ã„ã‚‹", "æ©Ÿèƒ½ã—ã¦ã„ã‚‹",
            "æ°—ã«å…¥ã£ã¦ã„ã‚‹", "ä½¿ã„ã‚„ã™ã„", "ã‚¹ãƒ ãƒ¼ã‚º",
            "ã†ã¾ã", "ã¡ã‚ƒã‚“ã¨", "ã—ã£ã‹ã‚Š", "å¿«é©",
        ],
        "patterns": [
            r"(.+?)(?:ã¯|ãŒ)(?:ã†ã¾ãã„ã£ã¦ã„ã‚‹|é †èª¿|æˆåŠŸ)",
            r"(.+?)(?:ã«|ã¯)(?:æº€è¶³|å•é¡Œãªã„)",
            r"(.+?)(?:ã¯|ãŒ)(?:ä¾¿åˆ©|åŠ©ã‹ã£ã¦ã„ã‚‹|åŠ¹ç‡çš„)",
        ],
    },
}

# æ¸©åº¦æ„Ÿã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿
TEMPERATURE_INDICATORS = {
    "high": ["çµ¶å¯¾", "æœ¬å½“ã«", "éå¸¸ã«", "ã¨ã¦ã‚‚", "ã™ã”ã", "ã‚ã¡ã‚ƒãã¡ã‚ƒ", "ã„ã¤ã‚‚", "æ¯å›", "å¿…ãš"],
    "medium": ["ã‹ãªã‚Š", "çµæ§‹", "ã‚ã‚Šã¨", "æ™‚ã€…", "ãŸã¾ã«", "ã‚ˆã"],
    "low": ["å°‘ã—", "ã¡ã‚‡ã£ã¨", "å¤šå°‘", "è‹¥å¹²", "ãŸã¶ã‚“", "ãŠãã‚‰ã"],
}


# ========================================
# å‹å®šç¾©
# ========================================

@dataclass
class PIVOTInsight:
    """PIVOTåˆ†é¡ã•ã‚ŒãŸã‚¤ãƒ³ã‚µã‚¤ãƒˆ"""
    id: str
    pivot_voice: str  # P, I, V, O, T
    pivot_label: str
    pivot_score: int
    title: str
    body: str
    confidence: float
    temperature: str
    matched_keywords: List[str] = field(default_factory=list)
    video_id: Optional[str] = None
    timestamp: Optional[float] = None  # å‹•ç”»å†…ã®æ™‚é–“ï¼ˆç§’ï¼‰


@dataclass
class PIVOTAnalysisResult:
    """PIVOTåˆ†æçµæœ"""
    items: List[PIVOTInsight]
    by_pivot: Dict[str, List[PIVOTInsight]]
    total_score: int
    sentiment_index: float
    stats: Dict


@dataclass
class VideoAnalysisResult:
    """å‹•ç”»å˜ä½ã®åˆ†æçµæœ"""
    video_id: str
    video_title: str
    channel: str
    analyzed_at: str
    pivot_result: PIVOTAnalysisResult

    @property
    def total_score(self) -> int:
        return self.pivot_result.total_score

    @property
    def sentiment_index(self) -> float:
        return self.pivot_result.sentiment_index

    @property
    def pain_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("P", []))

    @property
    def insecurity_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("I", []))

    @property
    def vision_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("V", []))

    @property
    def objection_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("O", []))

    @property
    def traction_count(self) -> int:
        return len(self.pivot_result.by_pivot.get("T", []))

    def to_dict(self) -> dict:
        return {
            "video_id": self.video_id,
            "video_title": self.video_title,
            "channel": self.channel,
            "analyzed_at": self.analyzed_at,
            "stats": {
                "total_insights": len(self.pivot_result.items),
                "total_score": self.total_score,
                "sentiment_index": round(self.sentiment_index, 2),
                "by_pivot": {
                    "P": self.pain_count,
                    "I": self.insecurity_count,
                    "V": self.vision_count,
                    "O": self.objection_count,
                    "T": self.traction_count,
                },
            },
            "items": [
                {
                    "id": item.id,
                    "pivot": item.pivot_voice,
                    "title": item.title,
                    "body": item.body,
                    "confidence": round(item.confidence, 2),
                    "temperature": item.temperature,
                    "keywords": item.matched_keywords,
                    "timestamp": item.timestamp,
                }
                for item in self.pivot_result.items
            ],
        }

    def to_mart_items(self, observed_at: Optional[str] = None) -> List[dict]:
        """PIVOT Martã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦å‡ºåŠ›"""
        observed_at = observed_at or datetime.now().strftime("%Y-%m-%d")
        marts = []

        for item in self.pivot_result.items:
            marts.append({
                "id": f"pivot_{item.id}",
                "mart_type": "pivot_insight",
                "pivot_voice": item.pivot_voice,
                "pivot_label": item.pivot_label,
                "pivot_score": item.pivot_score,
                "title": item.title,
                "body": item.body,
                "confidence": item.confidence,
                "temperature": item.temperature,
                "keywords": {"surface": item.matched_keywords},
                "source_ref": {
                    "doc_id": self.video_id,
                    "doc_type": "youtube_transcript",
                    "channel": self.channel,
                    "title": self.video_title,
                    "timestamp": item.timestamp,
                },
                "source_time": {"observed_at": observed_at},
            })

        return marts


# ========================================
# PIVOT Analyzer
# ========================================

class PIVOTAnalyzer:
    """YouTubeæ–‡å­—èµ·ã“ã—ã®PIVOTåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(
        self,
        domain: Optional[str] = None,
        min_confidence: float = 0.3,
        split_by_sentence: bool = True,
    ):
        """
        Args:
            domain: æ¥­å‹™ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆå°†æ¥ã®é‡ã¿ä»˜ã‘ç”¨ï¼‰
            min_confidence: æœ€å°ä¿¡é ¼åº¦é–¾å€¤
            split_by_sentence: å¥ç‚¹ã§æ–‡ã‚’åˆ†å‰²ã™ã‚‹ã‹
        """
        self.domain = domain
        self.min_confidence = min_confidence
        self.split_by_sentence = split_by_sentence

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        self.pivot_patterns = {}
        for pivot, config in PIVOT_KEYWORDS.items():
            self.pivot_patterns[pivot] = [
                re.compile(p) for p in config.get("patterns", [])
            ]

    def analyze_video(self, video: VideoData) -> VideoAnalysisResult:
        """
        å˜ä¸€å‹•ç”»ã‚’åˆ†æ

        Args:
            video: VideoDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

        Returns:
            VideoAnalysisResult: åˆ†æçµæœ
        """
        # æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        transcript_text = video.transcript.full_text if video.transcript else ""

        if not transcript_text:
            # æ–‡å­—èµ·ã“ã—ãŒãªã„å ´åˆã¯ç©ºã®çµæœã‚’è¿”ã™
            return VideoAnalysisResult(
                video_id=video.video_id,
                video_title=video.metadata.title,
                channel=video.metadata.channel,
                analyzed_at=datetime.now().isoformat(),
                pivot_result=PIVOTAnalysisResult(
                    items=[],
                    by_pivot={p: [] for p in PIVOT.ALL},
                    total_score=0,
                    sentiment_index=0.0,
                    stats={"total": 0, "by_pivot": {p: 0 for p in PIVOT.ALL}},
                ),
            )

        # æ–‡ã‚’åˆ†å‰²
        sentences = self._split_sentences(transcript_text)

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°
        timestamp_map = self._build_timestamp_map(video)

        # å„æ–‡ã‚’PIVOTåˆ†é¡
        items: List[PIVOTInsight] = []
        by_pivot: Dict[str, List[PIVOTInsight]] = {p: [] for p in PIVOT.ALL}

        for sentence in sentences:
            insight = self._classify_sentence(sentence, video.video_id, timestamp_map)
            if insight and insight.confidence >= self.min_confidence:
                items.append(insight)
                by_pivot[insight.pivot_voice].append(insight)

        # ã‚¹ã‚³ã‚¢ç®—å‡º
        total_score = sum(item.pivot_score for item in items)
        sentiment_index = total_score / len(items) if items else 0.0

        stats = {
            "total": len(items),
            "by_pivot": {p: len(lst) for p, lst in by_pivot.items()},
            "domain": self.domain,
        }

        return VideoAnalysisResult(
            video_id=video.video_id,
            video_title=video.metadata.title,
            channel=video.metadata.channel,
            analyzed_at=datetime.now().isoformat(),
            pivot_result=PIVOTAnalysisResult(
                items=items,
                by_pivot=by_pivot,
                total_score=total_score,
                sentiment_index=sentiment_index,
                stats=stats,
            ),
        )

    def analyze_videos(self, videos: List[VideoData]) -> List[VideoAnalysisResult]:
        """
        è¤‡æ•°å‹•ç”»ã‚’åˆ†æ

        Args:
            videos: VideoDataã®ãƒªã‚¹ãƒˆ

        Returns:
            List[VideoAnalysisResult]: åˆ†æçµæœãƒªã‚¹ãƒˆ
        """
        return [self.analyze_video(video) for video in videos]

    def _split_sentences(self, text: str) -> List[str]:
        """æ–‡ã‚’åˆ†å‰²"""
        if not self.split_by_sentence:
            return [text]

        # å¥ç‚¹ã€æ„Ÿå˜†ç¬¦ã€ç–‘å•ç¬¦ã€æ”¹è¡Œã§åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]+', text)
        return [s.strip() for s in sentences if s.strip() and len(s.strip()) >= 10]

    def _build_timestamp_map(self, video: VideoData) -> Dict[str, float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰"""
        timestamp_map = {}
        if video.transcript and video.transcript.segments:
            for seg in video.transcript.segments:
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®å…ˆé ­éƒ¨åˆ†ã‚’ã‚­ãƒ¼ã«ã™ã‚‹
                key = seg.text[:30] if len(seg.text) > 30 else seg.text
                timestamp_map[key] = seg.start
        return timestamp_map

    def _find_timestamp(self, text: str, timestamp_map: Dict[str, float]) -> Optional[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã™ã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ¢ã™"""
        for key, timestamp in timestamp_map.items():
            if key in text or text[:30] in key:
                return timestamp
        return None

    def _classify_sentence(
        self,
        text: str,
        video_id: str,
        timestamp_map: Dict[str, float],
    ) -> Optional[PIVOTInsight]:
        """å˜ä¸€ã®æ–‡ã‚’PIVOTåˆ†é¡"""
        if not text.strip():
            return None

        # å„PIVOTã‚«ãƒ†ã‚´ãƒªã«å¯¾ã—ã¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scores: Dict[str, Tuple[float, List[str]]] = {}

        for pivot in PIVOT.ALL:
            config = PIVOT_KEYWORDS[pivot]
            keywords = config["keywords"]
            patterns = self.pivot_patterns[pivot]

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            matched_kw = [kw for kw in keywords if kw in text]
            kw_score = min(len(matched_kw) * 0.25, 0.6)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            pat_score = 0.0
            for pattern in patterns:
                if pattern.search(text):
                    pat_score = 0.4
                    break

            # åˆè¨ˆã‚¹ã‚³ã‚¢
            total_score = min(kw_score + pat_score, 0.95)

            if total_score > 0:
                scores[pivot] = (total_score, matched_kw)

        if not scores:
            return None

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®PIVOTã‚’é¸æŠ
        best_pivot = max(scores.keys(), key=lambda p: scores[p][0])
        confidence, matched_keywords = scores[best_pivot]

        # æ¸©åº¦æ„Ÿåˆ¤å®š
        temperature = self._detect_temperature(text)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ¢ã™
        timestamp = self._find_timestamp(text, timestamp_map)

        return PIVOTInsight(
            id=str(uuid.uuid4()),
            pivot_voice=best_pivot,
            pivot_label=PIVOT.LABELS[best_pivot],
            pivot_score=PIVOT.SCORES[best_pivot],
            title=self._truncate(text, 50),
            body=text,
            confidence=confidence,
            temperature=temperature,
            matched_keywords=matched_keywords,
            video_id=video_id,
            timestamp=timestamp,
        )

    def _detect_temperature(self, text: str) -> str:
        """æ¸©åº¦æ„Ÿã‚’åˆ¤å®š"""
        for level, indicators in TEMPERATURE_INDICATORS.items():
            if any(ind in text for ind in indicators):
                return level
        return "medium"

    def _truncate(self, text: str, max_len: int) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚"""
        text = text.replace("\n", " ").strip()
        if len(text) <= max_len:
            return text
        return text[:max_len] + "..."


# ========================================
# ä¾¿åˆ©é–¢æ•°
# ========================================

def analyze_video(
    video: VideoData,
    domain: Optional[str] = None,
) -> VideoAnalysisResult:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ - å˜ä¸€å‹•ç”»åˆ†æ

    Args:
        video: VideoDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        domain: æ¥­å‹™ãƒ‰ãƒ¡ã‚¤ãƒ³

    Returns:
        VideoAnalysisResult: åˆ†æçµæœ
    """
    analyzer = PIVOTAnalyzer(domain=domain)
    return analyzer.analyze_video(video)


def analyze_videos(
    videos: List[VideoData],
    domain: Optional[str] = None,
) -> List[VideoAnalysisResult]:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ - è¤‡æ•°å‹•ç”»åˆ†æ

    Args:
        videos: VideoDataã®ãƒªã‚¹ãƒˆ
        domain: æ¥­å‹™ãƒ‰ãƒ¡ã‚¤ãƒ³

    Returns:
        List[VideoAnalysisResult]: åˆ†æçµæœãƒªã‚¹ãƒˆ
    """
    analyzer = PIVOTAnalyzer(domain=domain)
    return analyzer.analyze_videos(videos)


def save_analysis_results(
    results: List[VideoAnalysisResult],
    output_path: str,
    format: str = "json",
) -> None:
    """
    åˆ†æçµæœã‚’ä¿å­˜

    Args:
        results: VideoAnalysisResultã®ãƒªã‚¹ãƒˆ
        output_path: å‡ºåŠ›ãƒ‘ã‚¹
        format: å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ ("json" or "jsonl")
    """
    path = Path(output_path)
    path.parent.mkdir(parents=True, exist_ok=True)

    if format == "jsonl":
        with open(path, "w", encoding="utf-8") as f:
            for result in results:
                for mart in result.to_mart_items():
                    f.write(json.dumps(mart, ensure_ascii=False) + "\n")
    else:
        data = {
            "analyzed_at": datetime.now().isoformat(),
            "total_videos": len(results),
            "results": [r.to_dict() for r in results],
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


def print_analysis_summary(results: List[VideoAnalysisResult]) -> None:
    """åˆ†æã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "=" * 60)
    print("PIVOTåˆ†æã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    total_insights = sum(len(r.pivot_result.items) for r in results)
    total_score = sum(r.total_score for r in results)

    print(f"\nğŸ“Š åˆ†æå‹•ç”»æ•°: {len(results)}")
    print(f"ğŸ“ ç·ã‚¤ãƒ³ã‚µã‚¤ãƒˆæ•°: {total_insights}")
    print(f"ğŸ“ˆ ç·åˆã‚¹ã‚³ã‚¢: {total_score}")

    # PIVOTåˆ¥é›†è¨ˆ
    pivot_totals = {p: 0 for p in PIVOT.ALL}
    for result in results:
        for pivot in PIVOT.ALL:
            pivot_totals[pivot] += len(result.pivot_result.by_pivot.get(pivot, []))

    print("\nğŸ“‹ PIVOTåˆ†å¸ƒ:")
    print("-" * 40)
    pivot_labels = {
        "P": "Pain (èª²é¡Œ)",
        "I": "Insecurity (ä¸å®‰)",
        "V": "Vision (è¦æœ›)",
        "O": "Objection (æŠµæŠ—)",
        "T": "Traction (æˆåŠŸ)",
    }
    for pivot, label in pivot_labels.items():
        count = pivot_totals[pivot]
        bar = "â–ˆ" * min(count, 20)
        print(f"  {pivot} {label:20} {count:3}ä»¶ {bar}")

    # ä¸Šä½å‹•ç”»
    if results:
        print("\nğŸ¯ èª²é¡ŒãŒå¤šã„å‹•ç”» (Top 5):")
        print("-" * 40)
        sorted_by_pain = sorted(results, key=lambda r: r.pain_count, reverse=True)[:5]
        for r in sorted_by_pain:
            print(f"  [{r.pain_count}P] {r.video_title[:40]}")

    print("\n" + "=" * 60)
