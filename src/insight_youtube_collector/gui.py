"""
Streamlit GUI for Insight YouTube Collector.

Run with:
    streamlit run src/insight_youtube_collector/gui.py
    # or
    iyc-gui
"""

import streamlit as st
import os
import sys
from pathlib import Path
from datetime import datetime

# Add src to path for development
src_path = Path(__file__).parent.parent
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from insight_youtube_collector import __version__
from insight_youtube_collector.collector import YouTubeCollector
from insight_youtube_collector.batch import BatchConfig, BatchCollector
from insight_youtube_collector.storage import WarehouseStorage
from insight_youtube_collector.config import Settings


def init_session_state():
    """Initialize session state variables."""
    if 'collection_results' not in st.session_state:
        st.session_state.collection_results = []
    if 'collection_log' not in st.session_state:
        st.session_state.collection_log = []
    if 'search_results' not in st.session_state:
        st.session_state.search_results = []
    if 'selected_videos' not in st.session_state:
        st.session_state.selected_videos = set()


def log_message(message: str):
    """Add message to collection log."""
    timestamp = datetime.now().strftime("%H:%M:%S")
    st.session_state.collection_log.append(f"[{timestamp}] {message}")


def clear_log():
    """Clear collection log."""
    st.session_state.collection_log = []


def format_duration(seconds: int) -> str:
    """Format seconds as HH:MM:SS or MM:SS."""
    if not seconds:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"


class NullLogger:
    """Null logger to suppress all yt-dlp output."""
    def debug(self, msg): pass
    def info(self, msg): pass
    def warning(self, msg): pass
    def error(self, msg): pass


def search_videos(query: str, max_results: int):
    """Search for videos and store results in session state."""
    import time
    import os
    import sys
    log_message(f"æ¤œç´¢ä¸­: {query}")

    from insight_youtube_collector.extractor import VideoSourceExtractor
    import yt_dlp

    def fetch_video_info(vid: str, retry_count: int = 0) -> dict | None:
        """Fetch video info with retry on rate limit."""
        max_retries = 3
        url = f"https://www.youtube.com/watch?v={vid}"

        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'skip_download': True,
            'writesubtitles': False,
            'writeautomaticsub': False,
            'ignoreerrors': True,
            'no_color': True,
            'logger': NullLogger(),
        }

        # Try with cookies first (only on first attempt)
        if retry_count == 0:
            ydl_opts['cookiesfrombrowser'] = ('chrome',)

        # Suppress stderr completely by redirecting to devnull
        old_stderr = sys.stderr
        try:
            sys.stderr = open(os.devnull, 'w')
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
                if info:
                    return info
        except Exception as e:
            error_msg = str(e).lower()
            # If cookie error, retry without cookies
            if 'cookie' in error_msg and 'cookiesfrombrowser' in ydl_opts:
                del ydl_opts['cookiesfrombrowser']
                try:
                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        info = ydl.extract_info(url, download=False)
                        if info:
                            return info
                except Exception:
                    pass
            # If rate limited, retry with backoff
            if '429' in str(e) and retry_count < max_retries:
                wait_time = (retry_count + 1) * 10  # 10, 20, 30 seconds
                log_message(f"  Rate limited, waiting {wait_time}s...")
                time.sleep(wait_time)
                return fetch_video_info(vid, retry_count + 1)
        finally:
            sys.stderr.close()
            sys.stderr = old_stderr

        return None

    try:
        with st.spinner("ğŸ” æ¤œç´¢ä¸­..."):
            source_extractor = VideoSourceExtractor(quiet=True)

            # Get video IDs from search
            video_ids = source_extractor.extract_from_search(query, max_results)

            if not video_ids:
                st.warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                log_message("æ¤œç´¢çµæœãªã—")
                return

            # Get metadata and subtitle info for each video
            results = []
            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, vid in enumerate(video_ids):
                status_text.text(f"å­—å¹•æƒ…å ±ã‚’ç¢ºèªä¸­... {i+1}/{len(video_ids)}")
                progress_bar.progress((i + 1) / len(video_ids))

                try:
                    info = fetch_video_info(vid)
                    if not info:
                        log_message(f"  ã‚¹ã‚­ãƒƒãƒ—: {vid} - æƒ…å ±å–å¾—å¤±æ•—")
                        continue

                    # Check if subtitles are available
                    subtitles = info.get('subtitles', {})
                    auto_captions = info.get('automatic_captions', {})
                    has_ja = 'ja' in subtitles or 'ja' in auto_captions
                    has_en = 'en' in subtitles or 'en' in auto_captions
                    has_any = bool(subtitles) or bool(auto_captions)

                    results.append({
                        'video_id': vid,
                        'title': info.get('title', '(ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜)'),
                        'channel': info.get('channel', info.get('uploader', '')),
                        'duration': info.get('duration', 0),
                        'url': f"https://www.youtube.com/watch?v={vid}",
                        'has_subtitles': has_any,
                        'has_ja': has_ja,
                        'has_en': has_en,
                        'subtitle_langs': list(subtitles.keys()) + list(auto_captions.keys()),
                    })

                    # Add small delay between requests to avoid rate limiting
                    if i < len(video_ids) - 1:
                        time.sleep(1.5)

                except Exception as e:
                    log_message(f"  ã‚¹ã‚­ãƒƒãƒ—: {vid} - {e}")
                    continue

            progress_bar.empty()
            status_text.empty()

            # Filter to only videos with subtitles
            videos_with_subs = [r for r in results if r['has_subtitles']]

            st.session_state.search_results = videos_with_subs
            st.session_state.selected_videos = set()  # Clear previous selections

            total_found = len(results)
            with_subs = len(videos_with_subs)
            log_message(f"æ¤œç´¢å®Œäº†: {total_found} ä»¶ä¸­ {with_subs} ä»¶ãŒå­—å¹•ã‚ã‚Š")

            if videos_with_subs:
                st.success(f"âœ… {with_subs} ä»¶ã®å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆå­—å¹•ã‚ã‚Šï¼‰")
                if total_found > with_subs:
                    st.info(f"â„¹ï¸ å­—å¹•ãªã—ã® {total_found - with_subs} ä»¶ã¯é™¤å¤–ã•ã‚Œã¾ã—ãŸ")
            else:
                st.warning("å­—å¹•ã®ã‚ã‚‹å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            st.rerun()

    except Exception as e:
        st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        log_message(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")


def collect_selected_videos(video_ids: list, warehouse_dir: str, json_path: str):
    """Collect transcripts for selected videos."""
    clear_log()
    log_message(f"é¸æŠã—ãŸ {len(video_ids)} ä»¶ã®åé›†é–‹å§‹")

    settings = Settings(quiet_mode=True)
    collector = YouTubeCollector(settings, status_callback=log_message)

    progress = st.progress(0)
    status = st.empty()

    try:
        videos = []
        total = len(video_ids)

        for i, vid in enumerate(video_ids):
            status.info(f"ğŸ”„ [{i+1}/{total}] ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå–å¾—ä¸­...")
            log_message(f"å–å¾—ä¸­: {vid}")
            progress.progress((i + 1) / (total + 1))

            video_data = collector.collect_video(vid, verbose=False)
            if video_data:
                videos.append(video_data)
                if video_data.transcript and not video_data.transcript.error:
                    text_len = len(video_data.transcript.full_text)
                    seg_count = video_data.transcript.segment_count
                    log_message(f"  âœ“ {video_data.metadata.title} ({text_len}æ–‡å­—, {seg_count}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ)")
                else:
                    error_msg = video_data.transcript.error if video_data.transcript else "ä¸æ˜"
                    log_message(f"  âœ— {video_data.metadata.title} - {error_msg}")

        log_message(f"å–å¾—å®Œäº†: {len(videos)} å‹•ç”»")

        if videos:
            # Save to warehouse
            if warehouse_dir:
                status.info("ğŸ’¾ Warehouseã«ä¿å­˜ä¸­...")
                log_message("Warehouseã«ä¿å­˜ä¸­...")
                result = collector.save_warehouse(videos, warehouse_dir=warehouse_dir)
                saved = result['saved']
                skipped = result.get('skipped', 0)
                if skipped > 0:
                    log_message(f"Warehouseä¿å­˜: {saved} ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{skipped} ä»¶ã¯æ—¢å­˜ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                else:
                    log_message(f"Warehouseä¿å­˜: {saved} ãƒ•ã‚¡ã‚¤ãƒ«")

            # Save to JSON
            if json_path:
                log_message("JSONã«ä¿å­˜ä¸­...")
                collector.save_json(videos, output_path=json_path)
                log_message(f"JSONä¿å­˜: {json_path}")

            progress.progress(100)
            status.success(f"âœ… å®Œäº†: {len(videos)} å‹•ç”»ã‚’åé›†ã—ã¾ã—ãŸ")
            log_message("åé›†å®Œäº†!")

            # Clear search results
            st.session_state.search_results = []
            st.session_state.selected_videos = set()

            # Show results
            st.subheader("åé›†çµæœ")
            success_count = sum(1 for v in videos if v.transcript and not v.transcript.error)
            st.metric("ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå–å¾—æˆåŠŸ", f"{success_count} / {len(videos)}")

            for v in videos[:10]:
                has_transcript = v.transcript and not v.transcript.error
                icon = "âœ“" if has_transcript else "âœ—"
                st.write(f"{icon} **{v.metadata.title}**")
            if len(videos) > 10:
                st.write(f"... ä»– {len(videos) - 10} ä»¶")
        else:
            status.warning("å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            log_message("å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    except Exception as e:
        status.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        log_message(f"ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """Main Streamlit app."""
    st.set_page_config(
        page_title="Insight YouTube Collector",
        page_icon="ğŸ¬",
        layout="wide",
    )

    init_session_state()

    # Header
    st.title("ğŸ¬ Insight YouTube Collector")
    st.caption(f"v{__version__} - Harmonic Insight Text Data Warehouse Tool")

    # Sidebar - Settings
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")

        warehouse_dir = st.text_input(
            "Warehouse ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
            value="data/warehouse/lectures",
            help="åé›†ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
        )

        max_videos = st.slider(
            "æœ€å¤§å‹•ç”»æ•°ï¼ˆã‚½ãƒ¼ã‚¹ã”ã¨ï¼‰",
            min_value=1,
            max_value=100,
            value=10,
            help="å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§å‹•ç”»æ•°"
        )

        st.divider()

        # Output format
        st.subheader("å‡ºåŠ›å½¢å¼")
        save_warehouse = st.checkbox("Warehouseå½¢å¼ï¼ˆHMGç”¨ï¼‰", value=True)
        save_json = st.checkbox("JSONå½¢å¼", value=False)

        if save_json:
            json_path = st.text_input("JSONå‡ºåŠ›ãƒ‘ã‚¹", value="data/output/result.json")

        st.divider()

        # Warehouse status
        st.subheader("ğŸ“ Warehouse çŠ¶æ…‹")
        try:
            storage = WarehouseStorage(warehouse_dir=warehouse_dir)
            files = storage.list_files()
            st.metric("ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°", len(files))
        except Exception:
            st.info("WarehouseãŒå­˜åœ¨ã—ã¾ã›ã‚“")

    # Main content - Tabs
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ” æ¤œç´¢ï¼†é¸æŠ",
        "ğŸ”— å˜ä¸€åé›†",
        "ğŸ“‹ ãƒãƒƒãƒåé›†",
        "ğŸ“ Warehouse",
        "ğŸ§  ãƒãƒ¼ãƒˆç”Ÿæˆ",
        "ğŸ“œ ãƒ­ã‚°"
    ])

    # Tab 1: Search & Select
    with tab1:
        st.header("ğŸ” æ¤œç´¢ã—ã¦é¸æŠ")
        st.caption("æ¤œç´¢çµæœã‹ã‚‰å‹•ç”»ã‚’é¸ã‚“ã§ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å–å¾—ã—ã¾ã™")

        # Search section
        col1, col2 = st.columns([3, 1])
        with col1:
            search_query = st.text_input(
                "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                placeholder="å»ºè¨­DX AIæ´»ç”¨ ãªã©",
                key="search_query"
            )
        with col2:
            search_max = st.number_input("æ¤œç´¢ä»¶æ•°", min_value=5, max_value=50, value=20, key="search_max")

        if st.button("ğŸ” æ¤œç´¢", key="search_btn", type="primary"):
            if search_query:
                search_videos(search_query, search_max)
            else:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        # Display search results
        if st.session_state.search_results:
            st.divider()
            st.subheader(f"æ¤œç´¢çµæœ: {len(st.session_state.search_results)} ä»¶")

            # Select all / Deselect all buttons
            col1, col2, col3 = st.columns([1, 1, 2])
            with col1:
                if st.button("âœ… ã™ã¹ã¦é¸æŠ"):
                    for v in st.session_state.search_results:
                        vid = v['video_id']
                        st.session_state.selected_videos.add(vid)
                        st.session_state[f"chk_{vid}"] = True
                    st.rerun()
            with col2:
                if st.button("âŒ ã™ã¹ã¦è§£é™¤"):
                    for v in st.session_state.search_results:
                        vid = v['video_id']
                        st.session_state.selected_videos.discard(vid)
                        st.session_state[f"chk_{vid}"] = False
                    st.rerun()
            with col3:
                selected_count = len(st.session_state.selected_videos)
                st.write(f"é¸æŠä¸­: **{selected_count}** ä»¶")

            # Video list with checkboxes
            for v in st.session_state.search_results:
                vid = v['video_id']
                is_selected = vid in st.session_state.selected_videos

                col1, col2 = st.columns([0.05, 0.95])
                with col1:
                    if st.checkbox("é¸æŠ", value=is_selected, key=f"chk_{vid}", label_visibility="collapsed"):
                        st.session_state.selected_videos.add(vid)
                    else:
                        st.session_state.selected_videos.discard(vid)
                with col2:
                    duration = format_duration(v.get('duration', 0))
                    # Show subtitle language badges
                    lang_badges = []
                    if v.get('has_ja'):
                        lang_badges.append("ğŸ‡¯ğŸ‡µ")
                    if v.get('has_en'):
                        lang_badges.append("ğŸ‡ºğŸ‡¸")
                    lang_str = " ".join(lang_badges) if lang_badges else "ğŸ“"
                    st.markdown(f"**{v['title']}** {lang_str}  \n{v['channel']} â€¢ {duration}")

            st.divider()

            # Collect selected videos
            selected_count = len(st.session_state.selected_videos)
            if selected_count > 0:
                if st.button(f"ğŸš€ é¸æŠã—ãŸ {selected_count} ä»¶ã‚’åé›†", type="primary", use_container_width=True):
                    collect_selected_videos(
                        list(st.session_state.selected_videos),
                        warehouse_dir if save_warehouse else None,
                        json_path if save_json else None,
                    )
            else:
                st.info("åé›†ã™ã‚‹å‹•ç”»ã‚’é¸æŠã—ã¦ãã ã•ã„")

    # Tab 2: Single Collection
    with tab2:
        st.header("å˜ä¸€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®åé›†")

        col1, col2 = st.columns([2, 1])

        with col1:
            source_type = st.selectbox(
                "ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—",
                ["å‹•ç”»URL", "ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ", "ãƒãƒ£ãƒ³ãƒãƒ«", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"],
                key="single_source_type"
            )

            if source_type == "å‹•ç”»URL":
                source_value = st.text_area(
                    "YouTube URLï¼ˆè¤‡æ•°ã®å ´åˆã¯1è¡Œ1URLï¼‰",
                    height=100,
                    placeholder="https://www.youtube.com/watch?v=...",
                    key="single_urls"
                )
            elif source_type == "ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ":
                source_value = st.text_input(
                    "ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ URL",
                    placeholder="https://www.youtube.com/playlist?list=...",
                    key="single_playlist"
                )
            elif source_type == "ãƒãƒ£ãƒ³ãƒãƒ«":
                source_value = st.text_input(
                    "ãƒãƒ£ãƒ³ãƒãƒ« URL",
                    placeholder="https://www.youtube.com/@channelname",
                    key="single_channel"
                )
            else:
                source_value = st.text_input(
                    "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                    placeholder="å»ºè¨­DX AIæ´»ç”¨",
                    key="single_search"
                )

        with col2:
            st.write("")  # Spacer
            st.write("")
            if st.button("ğŸš€ åé›†é–‹å§‹", key="single_collect", type="primary", use_container_width=True):
                if source_value:
                    collect_single(
                        source_type,
                        source_value,
                        max_videos,
                        warehouse_dir if save_warehouse else None,
                        json_path if save_json else None,
                    )
                else:
                    st.warning("ã‚½ãƒ¼ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # Tab 3: Batch Collection
    with tab3:
        st.header("ãƒãƒƒãƒåé›†")

        batch_mode = st.radio(
            "å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰",
            ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ", "URLãƒªã‚¹ãƒˆ", "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«"],
            horizontal=True
        )

        if batch_mode == "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ":
            keywords_text = st.text_area(
                "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ1è¡Œ1ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰",
                height=200,
                placeholder="å»ºè¨­DX AIæ´»ç”¨\næ–½å·¥ç®¡ç† ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–\nBIM æ´»ç”¨äº‹ä¾‹",
                key="batch_keywords"
            )

            if st.button("ğŸš€ ãƒãƒƒãƒåé›†é–‹å§‹", key="batch_keywords_btn", type="primary"):
                if keywords_text.strip():
                    keywords = [k.strip() for k in keywords_text.strip().split('\n') if k.strip()]
                    collect_batch_keywords(
                        keywords,
                        max_videos,
                        warehouse_dir if save_warehouse else None,
                    )
                else:
                    st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        elif batch_mode == "URLãƒªã‚¹ãƒˆ":
            urls_text = st.text_area(
                "URLï¼ˆ1è¡Œ1URLã€ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ/ãƒãƒ£ãƒ³ãƒãƒ«/å‹•ç”»ã‚’æ··åœ¨å¯ï¼‰",
                height=200,
                placeholder="https://www.youtube.com/playlist?list=...\nhttps://www.youtube.com/@channelname\nhttps://www.youtube.com/watch?v=...",
                key="batch_urls"
            )

            if st.button("ğŸš€ ãƒãƒƒãƒåé›†é–‹å§‹", key="batch_urls_btn", type="primary"):
                if urls_text.strip():
                    urls = [u.strip() for u in urls_text.strip().split('\n') if u.strip()]
                    collect_batch_urls(
                        urls,
                        max_videos,
                        warehouse_dir if save_warehouse else None,
                    )
                else:
                    st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        else:  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
            config_file = st.file_uploader(
                "YAML/JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«",
                type=['yaml', 'yml', 'json'],
                key="batch_config_file"
            )

            if config_file and st.button("ğŸš€ ãƒãƒƒãƒåé›†é–‹å§‹", key="batch_config_btn", type="primary"):
                collect_batch_config(config_file, warehouse_dir)

    # Tab 4: Warehouse Browser
    with tab4:
        st.header("Warehouse ãƒ–ãƒ©ã‚¦ã‚¶")

        try:
            storage = WarehouseStorage(warehouse_dir=warehouse_dir)
            files = storage.list_files()
            manifest = storage.get_manifest()

            if files:
                st.success(f"ğŸ“ {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")

                # File list with search
                search = st.text_input("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«åã§æ¤œç´¢", key="warehouse_search")

                filtered_files = files
                if search:
                    filtered_files = [f for f in files if search.lower() in f.lower()]

                for f in filtered_files[:50]:  # Limit display
                    meta = manifest.get("files", {}).get(f, {})
                    title = meta.get("source_title", "")
                    channel = meta.get("channel", "")

                    with st.expander(f"ğŸ“„ {f[:60]}..."):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**ã‚¿ã‚¤ãƒˆãƒ«:** {title}")
                            st.write(f"**ãƒãƒ£ãƒ³ãƒãƒ«:** {channel}")
                        with col2:
                            st.write(f"**åé›†æ—¥:** {meta.get('observed_at', 'N/A')}")
                            st.write(f"**å…¬é–‹æ—¥:** {meta.get('upload_date', 'N/A')}")

                        # Read and display file content preview
                        file_path = Path(warehouse_dir) / f
                        if file_path.exists():
                            content = file_path.read_text(encoding='utf-8')
                            file_size = file_path.stat().st_size
                            st.caption(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes / {len(content):,} æ–‡å­—")
                            if len(content) > 2000:
                                st.text_area("å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (æœ€åˆã®2000æ–‡å­—)", content[:2000] + "\n\n... (ä»¥ä¸‹çœç•¥)", height=200, disabled=True)
                            else:
                                st.text_area("å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", content, height=200, disabled=True)

                if len(filtered_files) > 50:
                    st.info(f"... ä»– {len(filtered_files) - 50} ãƒ•ã‚¡ã‚¤ãƒ«")

            else:
                st.info("Warehouseã¯ç©ºã§ã™ã€‚åé›†ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

        except Exception as e:
            st.error(f"Warehouseèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # Tab 5: PIVOT Mart Generation
    with tab5:
        st.header("ğŸ§  ãƒãƒ¼ãƒˆç”Ÿæˆ")
        st.caption("Warehouseã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åˆ†æã—ã¦ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™")

        # PIVOT Mart Type Selection
        st.subheader("ğŸ“Š ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—é¸æŠ")

        # PIVOT Voice Types
        pivot_mart_types = {
            "pain": {
                "label": "ğŸ˜° Pain (èª²é¡Œãƒ»å›°ã‚Šã”ã¨)",
                "desc": "ã€Œå›°ã£ã¦ã„ã‚‹ã€ã€Œã§ããªã„ã€ã€Œå•é¡ŒãŒã‚ã‚‹ã€ãªã©ã®èª²é¡Œã‚’æŠ½å‡º",
                "score": -2,
            },
            "insecurity": {
                "label": "ğŸ˜Ÿ Insecurity (ä¸å®‰ãƒ»å¿ƒé…)",
                "desc": "ã€Œä¸å®‰ã€ã€Œå¿ƒé…ã€ã€Œãƒªã‚¹ã‚¯ã€ãªã©ã®å°†æ¥ã¸ã®æ‡¸å¿µã‚’æŠ½å‡º",
                "score": -1,
            },
            "vision": {
                "label": "âœ¨ Vision (è¦æœ›ãƒ»ç†æƒ³åƒ)",
                "desc": "ã€Œã—ãŸã„ã€ã€Œæ¬²ã—ã„ã€ã€Œç†æƒ³ã€ãªã©ã®æ”¹å–„è¦æœ›ã‚’æŠ½å‡º",
                "score": 1,
            },
            "objection": {
                "label": "ğŸš« Objection (æ‘©æ“¦ãƒ»æŠµæŠ—)",
                "desc": "ã€Œå«Œã ã€ã€Œåå¯¾ã€ã€Œç„¡ç†ã€ãªã©ã®å®Ÿè¡Œéšœå£ã‚’æŠ½å‡º",
                "score": -1,
            },
            "traction": {
                "label": "ğŸ‰ Traction (æˆåŠŸãƒ»å¼·ã¿)",
                "desc": "ã€Œã§ããŸã€ã€Œã†ã¾ãã„ã£ãŸã€ã€Œä¾¿åˆ©ã€ãªã©ã®æˆåŠŸäº‹ä¾‹ã‚’æŠ½å‡º",
                "score": 2,
            },
        }

        # Domain selection
        domain_options = {
            "requirements": "è¦ä»¶å®šç¾©ãƒ»èª²é¡Œç™ºè¦‹",
            "biz_analysis": "ãƒ“ã‚¸ãƒã‚¹åˆ†æãƒ»ç¾çŠ¶æŠŠæ¡",
            "hr_evaluation": "äººäº‹è©•ä¾¡ãƒ»1on1",
            "daily_concerns": "æ—¥å¸¸ã®å¿ƒé…äº‹ãƒ»ç›¸è«‡",
            "customer_voice": "é¡§å®¢ã®å£°ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯",
            "retrospective": "æŒ¯ã‚Šè¿”ã‚Šãƒ»ãƒ¬ãƒˆãƒ­ã‚¹ãƒšã‚¯ãƒ†ã‚£ãƒ–",
        }

        col1, col2 = st.columns(2)
        with col1:
            selected_domain = st.selectbox(
                "åˆ†æãƒ‰ãƒ¡ã‚¤ãƒ³",
                options=list(domain_options.keys()),
                format_func=lambda x: domain_options[x],
                help="ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ã‚ˆã£ã¦PIVOT Voiceã®é‡ã¿ä»˜ã‘ãŒå¤‰ã‚ã‚Šã¾ã™"
            )

        with col2:
            selected_marts = st.multiselect(
                "ç”Ÿæˆã™ã‚‹ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—",
                options=list(pivot_mart_types.keys()),
                default=["pain", "vision"],
                format_func=lambda x: pivot_mart_types[x]["label"],
            )

        # Show selected mart descriptions
        if selected_marts:
            st.markdown("**é¸æŠã—ãŸãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—:**")
            for mart in selected_marts:
                info = pivot_mart_types[mart]
                st.markdown(f"- {info['label']}: {info['desc']}")

        st.divider()

        # Source selection
        st.subheader("ğŸ“‚ åˆ†æå¯¾è±¡")

        try:
            storage = WarehouseStorage(warehouse_dir=warehouse_dir)
            files = storage.list_files()

            if files:
                # File selection
                analyze_all = st.checkbox("ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ", value=True)

                if not analyze_all:
                    selected_files = st.multiselect(
                        "åˆ†æã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«",
                        options=files,
                        default=files[:5] if len(files) > 5 else files,
                    )
                else:
                    selected_files = files

                st.info(f"ğŸ“ {len(selected_files)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æå¯¾è±¡ã«é¸æŠ")

                # Output options
                st.subheader("ğŸ’¾ å‡ºåŠ›è¨­å®š")
                col1, col2 = st.columns(2)
                with col1:
                    output_format = st.selectbox(
                        "å‡ºåŠ›å½¢å¼",
                        ["JSONL (HMGäº’æ›)", "JSON"],
                    )
                with col2:
                    output_path = st.text_input(
                        "å‡ºåŠ›ãƒ‘ã‚¹",
                        value=f"data/marts/pivot_{selected_domain}.jsonl"
                    )

                # Generate button
                if st.button("ğŸš€ PIVOTåˆ†æå®Ÿè¡Œ", type="primary", use_container_width=True):
                    if selected_files and selected_marts:
                        generate_pivot_marts(
                            warehouse_dir,
                            selected_files,
                            selected_marts,
                            selected_domain,
                            output_path,
                            output_format,
                        )
                    else:
                        st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„")
            else:
                st.info("Warehouseã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«å‹•ç”»ã‚’åé›†ã—ã¦ãã ã•ã„ã€‚")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    # Tab 6: Log
    with tab6:
        st.header("åé›†ãƒ­ã‚°")

        col1, col2 = st.columns([4, 1])
        with col2:
            if st.button("ğŸ—‘ï¸ ãƒ­ã‚°ã‚¯ãƒªã‚¢"):
                clear_log()
                st.rerun()

        if st.session_state.collection_log:
            log_text = "\n".join(st.session_state.collection_log)
            st.code(log_text, language=None)
        else:
            st.info("ãƒ­ã‚°ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“")


def collect_single(source_type, source_value, max_videos, warehouse_dir, json_path):
    """Collect from a single source."""
    clear_log()
    log_message(f"åé›†é–‹å§‹: {source_type}")

    settings = Settings(quiet_mode=True)
    collector = YouTubeCollector(settings, status_callback=log_message)

    progress = st.progress(0)
    status = st.empty()

    try:
        status.info("ğŸ”„ å‹•ç”»æƒ…å ±ã‚’å–å¾—ä¸­...")
        log_message("å‹•ç”»æƒ…å ±ã‚’å–å¾—ä¸­...")

        videos = []
        if source_type == "å‹•ç”»URL":
            urls = [u.strip() for u in source_value.strip().split('\n') if u.strip()]
            videos = collector.collect_from_urls(urls, max_videos=max_videos, verbose=False)
        elif source_type == "ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ":
            videos = collector.collect_from_playlist(source_value, max_videos=max_videos, verbose=False)
        elif source_type == "ãƒãƒ£ãƒ³ãƒãƒ«":
            videos = collector.collect_from_channel(source_value, max_videos=max_videos, verbose=False)
        else:
            videos = collector.collect_from_search(source_value, max_videos=max_videos, verbose=False)

        progress.progress(50)
        log_message(f"å–å¾—å®Œäº†: {len(videos)} å‹•ç”»")

        if videos:
            # Save to warehouse
            if warehouse_dir:
                status.info("ğŸ’¾ Warehouseã«ä¿å­˜ä¸­...")
                log_message("Warehouseã«ä¿å­˜ä¸­...")
                result = collector.save_warehouse(videos, warehouse_dir=warehouse_dir)
                saved = result['saved']
                skipped = result.get('skipped', 0)
                if skipped > 0:
                    log_message(f"Warehouseä¿å­˜: {saved} ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{skipped} ä»¶ã¯æ—¢å­˜ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                else:
                    log_message(f"Warehouseä¿å­˜: {saved} ãƒ•ã‚¡ã‚¤ãƒ«")

            # Save to JSON
            if json_path:
                log_message("JSONã«ä¿å­˜ä¸­...")
                collector.save_json(videos, output_path=json_path)
                log_message(f"JSONä¿å­˜: {json_path}")

            progress.progress(100)
            status.success(f"âœ… å®Œäº†: {len(videos)} å‹•ç”»ã‚’åé›†ã—ã¾ã—ãŸ")
            log_message("åé›†å®Œäº†!")

            # Show results
            st.subheader("åé›†çµæœ")
            for v in videos[:10]:
                st.write(f"- **{v.metadata.title}** ({v.metadata.channel})")
            if len(videos) > 10:
                st.write(f"... ä»– {len(videos) - 10} ä»¶")

        else:
            status.warning("å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            log_message("å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    except Exception as e:
        status.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        log_message(f"ã‚¨ãƒ©ãƒ¼: {e}")


def collect_batch_keywords(keywords, max_videos, warehouse_dir):
    """Batch collect from keywords."""
    clear_log()
    log_message(f"ãƒãƒƒãƒåé›†é–‹å§‹: {len(keywords)} ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

    from insight_youtube_collector.batch import BatchConfig, SourceConfig, BatchCollector

    # Build config
    sources = [SourceConfig("keyword", kw, max_videos) for kw in keywords]
    config = BatchConfig(
        sources=sources,
        save_warehouse=bool(warehouse_dir),
        warehouse_dir=warehouse_dir or "data/warehouse/lectures",
        save_json=False,
    )

    progress = st.progress(0)
    status = st.empty()

    try:
        total = len(keywords)
        for i, kw in enumerate(keywords):
            status.info(f"ğŸ”„ [{i+1}/{total}] æ¤œç´¢ä¸­: {kw}")
            log_message(f"æ¤œç´¢ä¸­: {kw}")
            progress.progress((i + 1) / (total + 1))

        status.info("ğŸ”„ åé›†å®Ÿè¡Œä¸­...")
        collector = BatchCollector(config, verbose=False)
        result = collector.collect_all()

        progress.progress(100)
        log_message(f"åé›†å®Œäº†: {result['unique_videos']} å‹•ç”»")

        status.success(f"âœ… å®Œäº†: {result['unique_videos']} å‹•ç”»ã‚’åé›†")

        # Show summary
        st.metric("åé›†å‹•ç”»æ•°", result['unique_videos'])
        if result.get('save_results', {}).get('warehouse'):
            wh = result['save_results']['warehouse']
            st.metric("ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°", wh['saved'])

    except Exception as e:
        status.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        log_message(f"ã‚¨ãƒ©ãƒ¼: {e}")


def collect_batch_urls(urls, max_videos, warehouse_dir):
    """Batch collect from URLs."""
    clear_log()
    log_message(f"ãƒãƒƒãƒåé›†é–‹å§‹: {len(urls)} URL")

    from insight_youtube_collector.batch import BatchConfig, SourceConfig, BatchCollector

    # Auto-detect source types
    sources = []
    for url in urls:
        if 'playlist?list=' in url:
            sources.append(SourceConfig("playlist", url, max_videos))
        elif '/@' in url or '/channel/' in url or '/c/' in url:
            sources.append(SourceConfig("channel", url, max_videos))
        else:
            sources.append(SourceConfig("url", url, 1))

    config = BatchConfig(
        sources=sources,
        save_warehouse=bool(warehouse_dir),
        warehouse_dir=warehouse_dir or "data/warehouse/lectures",
        save_json=False,
    )

    progress = st.progress(0)
    status = st.empty()

    try:
        status.info("ğŸ”„ åé›†å®Ÿè¡Œä¸­...")
        collector = BatchCollector(config, verbose=False)
        result = collector.collect_all()

        progress.progress(100)
        log_message(f"åé›†å®Œäº†: {result['unique_videos']} å‹•ç”»")

        status.success(f"âœ… å®Œäº†: {result['unique_videos']} å‹•ç”»ã‚’åé›†")

        st.metric("åé›†å‹•ç”»æ•°", result['unique_videos'])

    except Exception as e:
        status.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        log_message(f"ã‚¨ãƒ©ãƒ¼: {e}")


def collect_batch_config(config_file, warehouse_dir):
    """Batch collect from config file."""
    import tempfile
    import yaml
    import json

    clear_log()
    log_message("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒãƒƒãƒåé›†é–‹å§‹")

    progress = st.progress(0)
    status = st.empty()

    try:
        # Save uploaded file temporarily
        content = config_file.read().decode('utf-8')

        if config_file.name.endswith('.json'):
            data = json.loads(content)
        else:
            import yaml
            data = yaml.safe_load(content)

        config = BatchConfig.from_dict(data)
        log_message(f"è¨­å®šèª­ã¿è¾¼ã¿: {len(config.sources)} ã‚½ãƒ¼ã‚¹")

        status.info("ğŸ”„ åé›†å®Ÿè¡Œä¸­...")
        collector = BatchCollector(config, verbose=False)
        result = collector.collect_all()

        progress.progress(100)
        log_message(f"åé›†å®Œäº†: {result['unique_videos']} å‹•ç”»")

        status.success(f"âœ… å®Œäº†: {result['unique_videos']} å‹•ç”»ã‚’åé›†")

        st.metric("åé›†å‹•ç”»æ•°", result['unique_videos'])

    except Exception as e:
        status.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        log_message(f"ã‚¨ãƒ©ãƒ¼: {e}")


def generate_pivot_marts(warehouse_dir, files, mart_types, domain, output_path, output_format):
    """Generate PIVOT marts from warehouse files."""
    import json
    from pathlib import Path
    from .analyzer import PIVOTAnalyzer, PIVOTInsight
    from .models.video import VideoData, VideoMetadata, TranscriptData

    clear_log()
    log_message(f"PIVOTåˆ†æé–‹å§‹: {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«, ãƒ‰ãƒ¡ã‚¤ãƒ³={domain}")
    log_message(f"ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—: {', '.join(mart_types)}")

    progress = st.progress(0)
    status = st.empty()

    try:
        # Initialize analyzer
        analyzer = PIVOTAnalyzer(domain=domain, use_morphology=True)
        all_marts = []

        storage = WarehouseStorage(warehouse_dir=warehouse_dir)
        manifest = storage.get_manifest()

        for i, filename in enumerate(files):
            status.info(f"ğŸ”„ [{i+1}/{len(files)}] åˆ†æä¸­: {filename[:50]}...")
            progress.progress((i + 1) / (len(files) + 1))

            file_path = Path(warehouse_dir) / filename
            if not file_path.exists():
                log_message(f"  ã‚¹ã‚­ãƒƒãƒ—: {filename} - ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                continue

            # Read transcript
            content = file_path.read_text(encoding='utf-8')

            # Get metadata from manifest
            file_meta = manifest.get("files", {}).get(filename, {})
            video_id = file_meta.get("video_id", filename.replace(".txt", ""))
            title = file_meta.get("source_title", filename)
            channel = file_meta.get("channel", "Unknown")

            # Create minimal VideoData for analysis
            metadata = VideoMetadata(
                title=title,
                channel=channel,
                duration=0,
                view_count=0,
                upload_date="",
                description="",
                thumbnail_url="",
            )
            transcript = TranscriptData(
                language="ja",
                is_generated=True,
                segments=[],
                full_text=content,
            )
            video = VideoData(
                video_id=video_id,
                url=f"https://www.youtube.com/watch?v={video_id}",
                crawled_at=datetime.now(),
                metadata=metadata,
                transcript=transcript,
            )

            # Analyze
            result = analyzer.analyze_video(video)

            # Filter by selected mart types
            for item in result.pivot_result.items:
                voice_to_mart = {
                    "P": "pain",
                    "I": "insecurity",
                    "V": "vision",
                    "O": "objection",
                    "T": "traction",
                }
                item_mart_type = voice_to_mart.get(item.pivot_voice, "pain")
                if item_mart_type in mart_types:
                    all_marts.append({
                        "id": f"{item_mart_type}_{item.id}",
                        "mart_type": item_mart_type,
                        "pivot_voice": item.pivot_voice,
                        "pivot_label": item.pivot_label,
                        "pivot_score": item.pivot_score,
                        "target_layers": item.target_layers,
                        "title": item.title,
                        "body": item.body,
                        "confidence": item.confidence,
                        "temperature": item.temperature,
                        "keywords": {"surface": item.matched_keywords},
                        "source_ref": {
                            "doc_id": video_id,
                            "doc_type": "youtube_transcript",
                            "channel": channel,
                            "title": title,
                        },
                        "source_time": {
                            "observed_at": file_meta.get("observed_at", datetime.now().strftime("%Y-%m-%d"))
                        },
                        "morphology": {
                            "intensity_score": round(item.intensity_score, 2),
                            "degree_factor": item.degree_factor,
                            "certainty": item.certainty,
                            "reasoning": item.reasoning,
                        },
                    })

            log_message(f"  âœ“ {title[:40]}... ({len(result.pivot_result.items)} items)")

        # Save results
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        if "JSONL" in output_format:
            with open(output_file, "w", encoding="utf-8") as f:
                for mart in all_marts:
                    f.write(json.dumps(mart, ensure_ascii=False) + "\n")
        else:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump({
                    "domain": domain,
                    "mart_types": mart_types,
                    "total_items": len(all_marts),
                    "items": all_marts,
                }, f, ensure_ascii=False, indent=2)

        progress.progress(100)
        log_message(f"å®Œäº†: {len(all_marts)} ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç”Ÿæˆ")
        status.success(f"âœ… å®Œäº†: {len(all_marts)} ã‚¢ã‚¤ãƒ†ãƒ ã‚’ {output_path} ã«ä¿å­˜")

        # Show summary by mart type
        st.subheader("ğŸ“Š ç”Ÿæˆçµæœã‚µãƒãƒªãƒ¼")
        summary = {}
        for mart in all_marts:
            mt = mart["mart_type"]
            summary[mt] = summary.get(mt, 0) + 1

        cols = st.columns(len(summary) if summary else 1)
        for i, (mt, count) in enumerate(summary.items()):
            with cols[i]:
                labels = {
                    "pain": "ğŸ˜° Pain",
                    "insecurity": "ğŸ˜Ÿ Insecurity",
                    "vision": "âœ¨ Vision",
                    "objection": "ğŸš« Objection",
                    "traction": "ğŸ‰ Traction",
                }
                st.metric(labels.get(mt, mt), count)

        # Show sample items
        if all_marts:
            st.subheader("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ")
            for mart in all_marts[:5]:
                with st.expander(f"{mart['pivot_label']}: {mart['title'][:60]}..."):
                    st.write(f"**æœ¬æ–‡:** {mart['body'][:200]}...")
                    st.write(f"**å¼·åº¦:** {mart['morphology']['intensity_score']}")
                    st.write(f"**ç¢ºä¿¡åº¦:** {mart['morphology']['certainty']}")
                    st.write(f"**ã‚½ãƒ¼ã‚¹:** {mart['source_ref']['title']}")

    except Exception as e:
        status.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        log_message(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        log_message(traceback.format_exc())


if __name__ == "__main__":
    main()
